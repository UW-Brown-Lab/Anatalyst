{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Anatalyst: The Analysis Catalyst - A Modular Pipeline built for Single-cell RNA-seq Analysis \u00b6 Anatalyst is a flexible, modular Python pipeline designed to facilitate boilerplate analytical workflows that leverage existing Python and R programs. The current scope of module support is focused on Single Cell RNA sequencing, built on top of Scanpy , providing a customizable workflow for common single-cell analysis tasks. Key Features \u00b6 Modular Architecture : Each analysis step is encapsulated in a module that can be included or excluded as needed Configurable Pipeline : Simple YAML configuration to customize pipeline parameters Checkpoint System : Save and resume pipeline execution from checkpoints R Integration : Seamless integration of R tools (like SoupX) for specialized analyses Reproducible Analysis : Detailed reports with visualizations and parameter settings Framework Flexibility : New modules can be custom-built and inserted into the pipeline to allow for any type of sequential analysis Workflow Overview \u00b6 Anatalyst provides a comprehensive workflow for single-cell analysis: Data Loading : Import aligned 10X single-cell data using an .h5 file Quality Control : Calculate QC metrics and visualize distributions Ambient RNA Removal : Remove background RNA contamination using SoupX Doublet Detection : Identify and flag potential cell doublets Cell Filtering : Filter out low-quality cells and outliers Pearson Normalization : Normalize data using Pearson residuals Dimensionality Reduction : PCA, UMAP, and t-SNE for visualization and analysis Report Generation : Create comprehensive HTML reports with key figures Quick Start \u00b6 TBD - expecting pull Docker container with pipeline already installed Mount local directory for input/output and extra module insertion? # Install the container docker pull something-or-other # Run a pipeline with a configuration file python -m sc_pipeline.scripts.run_pipeline --config my_config.yaml # This command will likely change and may just be the way the container is launched? # Perhaps these args just get passed to docker compose via ENV variables and we make a new entrypoint.sh file? Check out the Getting Started guide for more detailed instructions. Pipeline Diagram \u00b6 insert diagram here","title":"Home"},{"location":"#anatalyst-the-analysis-catalyst-a-modular-pipeline-built-for-single-cell-rna-seq-analysis","text":"Anatalyst is a flexible, modular Python pipeline designed to facilitate boilerplate analytical workflows that leverage existing Python and R programs. The current scope of module support is focused on Single Cell RNA sequencing, built on top of Scanpy , providing a customizable workflow for common single-cell analysis tasks.","title":"Anatalyst: The Analysis Catalyst - A Modular Pipeline built for Single-cell RNA-seq Analysis"},{"location":"#key-features","text":"Modular Architecture : Each analysis step is encapsulated in a module that can be included or excluded as needed Configurable Pipeline : Simple YAML configuration to customize pipeline parameters Checkpoint System : Save and resume pipeline execution from checkpoints R Integration : Seamless integration of R tools (like SoupX) for specialized analyses Reproducible Analysis : Detailed reports with visualizations and parameter settings Framework Flexibility : New modules can be custom-built and inserted into the pipeline to allow for any type of sequential analysis","title":"Key Features"},{"location":"#workflow-overview","text":"Anatalyst provides a comprehensive workflow for single-cell analysis: Data Loading : Import aligned 10X single-cell data using an .h5 file Quality Control : Calculate QC metrics and visualize distributions Ambient RNA Removal : Remove background RNA contamination using SoupX Doublet Detection : Identify and flag potential cell doublets Cell Filtering : Filter out low-quality cells and outliers Pearson Normalization : Normalize data using Pearson residuals Dimensionality Reduction : PCA, UMAP, and t-SNE for visualization and analysis Report Generation : Create comprehensive HTML reports with key figures","title":"Workflow Overview"},{"location":"#quick-start","text":"TBD - expecting pull Docker container with pipeline already installed Mount local directory for input/output and extra module insertion? # Install the container docker pull something-or-other # Run a pipeline with a configuration file python -m sc_pipeline.scripts.run_pipeline --config my_config.yaml # This command will likely change and may just be the way the container is launched? # Perhaps these args just get passed to docker compose via ENV variables and we make a new entrypoint.sh file? Check out the Getting Started guide for more detailed instructions.","title":"Quick Start"},{"location":"#pipeline-diagram","text":"insert diagram here","title":"Pipeline Diagram"},{"location":"getting-started/","text":"Getting Started \u00b6 This guide will walk you through running your first analysis with Anatalyst. Basic Usage \u00b6 Anatalyst is designed to be run from a configuration file that defines the modules to use and their parameters. The basic workflow is: Create a configuration file Run the pipeline Review the results Creating a Configuration File \u00b6 Create a YAML file that defines your pipeline. Here's a minimal example: pipeline : name : my_first_pipeline output_dir : ./output r_memory_limit_gb : 8 figure_defaults : width : 8 height : 6 modules : - name : data_loading type : DataLoading params : file_path : /path/to/your/filtered_feature_bc_matrix.h5 - name : qc_metrics type : QCMetrics params : mito_pattern : \"^MT-\" ribo_pattern : \"^RP[SL]\" - name : report_generator type : ReportGenerator Save this as minimal_config.yaml . Running the Pipeline \u00b6 Execute the pipeline using the run_pipeline.py script: python -m /workspace/scripts/run_pipeline.py --config minimal_config.yaml This will: Load your data Calculate QC metrics Generate a report in the output directory Example with a Complete Analysis \u00b6 Here's a more comprehensive example configuration for a full analysis workflow: pipeline : name : complete_analysis output_dir : ./output r_memory_limit_gb : 8 figure_defaults : width : 8 height : 6 checkpointing : enabled : true modules_to_checkpoint : all max_checkpoints : 5 modules : - name : data_loading type : DataLoading params : file_path : /path/to/filtered_feature_bc_matrix.h5 - name : pre_qc type : QCMetrics params : mito_pattern : \"^MT-\" ribo_pattern : \"^RP[SL]\" create_plots : true - name : ambient_removal type : AmbientRNARemoval params : raw_counts_path : /path/to/raw_feature_bc_matrix.h5 filtered_counts_path : /path/to/filtered_feature_bc_matrix.h5 ndims : 30 resolution : 0.8 - name : doublet_detection type : DoubletDetection params : expected_doublet_rate : 0.05 - name : filtering type : Filtering params : filters : n_genes_by_counts : type : numeric pct_counts_mt : type : numeric predicted_doublet : type : boolean - name : post_qc type : QCMetrics params : mito_pattern : \"^MT-\" ribo_pattern : \"^RP[SL]\" - name : normalization type : PearsonNormalization params : n_top_genes : 2000 - name : dim_reduction type : DimensionalityReduction params : n_pcs : 50 compute_umap : true compute_tsne : true - name : report_generator type : ReportGenerator params : generate_html : true Save this as complete_config.yaml and run it as before: python -m /workspace/scripts/run_pipeline.py --config complete_config.yaml Resuming from a Checkpoint \u00b6 If your pipeline fails or stops for any reason, you can resume from the last checkpoint: python -m /workspace/scripts/run_pipeline.py --config complete_config.yaml --checkpoint doublet_detection This will resume the pipeline from after the \"doublet_detection\" module. Viewing the Results \u00b6 After running the pipeline, check the output directory: output/ \u251c\u2500\u2500 checkpoints/ # Pipeline checkpoints \u251c\u2500\u2500 images/ # Generated figures \u251c\u2500\u2500 analysis_report.md # Markdown report \u2514\u2500\u2500 analysis_report.html # HTML report Open the HTML report in a web browser to view the analysis results, including visualizations and parameter settings. Next Steps \u00b6 Explore the Configuration documentation to learn about all available options Browse the Modules section to understand each analysis step in detail Check out the Examples for more use cases and sample analyses","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"This guide will walk you through running your first analysis with Anatalyst.","title":"Getting Started"},{"location":"getting-started/#basic-usage","text":"Anatalyst is designed to be run from a configuration file that defines the modules to use and their parameters. The basic workflow is: Create a configuration file Run the pipeline Review the results","title":"Basic Usage"},{"location":"getting-started/#creating-a-configuration-file","text":"Create a YAML file that defines your pipeline. Here's a minimal example: pipeline : name : my_first_pipeline output_dir : ./output r_memory_limit_gb : 8 figure_defaults : width : 8 height : 6 modules : - name : data_loading type : DataLoading params : file_path : /path/to/your/filtered_feature_bc_matrix.h5 - name : qc_metrics type : QCMetrics params : mito_pattern : \"^MT-\" ribo_pattern : \"^RP[SL]\" - name : report_generator type : ReportGenerator Save this as minimal_config.yaml .","title":"Creating a Configuration File"},{"location":"getting-started/#running-the-pipeline","text":"Execute the pipeline using the run_pipeline.py script: python -m /workspace/scripts/run_pipeline.py --config minimal_config.yaml This will: Load your data Calculate QC metrics Generate a report in the output directory","title":"Running the Pipeline"},{"location":"getting-started/#example-with-a-complete-analysis","text":"Here's a more comprehensive example configuration for a full analysis workflow: pipeline : name : complete_analysis output_dir : ./output r_memory_limit_gb : 8 figure_defaults : width : 8 height : 6 checkpointing : enabled : true modules_to_checkpoint : all max_checkpoints : 5 modules : - name : data_loading type : DataLoading params : file_path : /path/to/filtered_feature_bc_matrix.h5 - name : pre_qc type : QCMetrics params : mito_pattern : \"^MT-\" ribo_pattern : \"^RP[SL]\" create_plots : true - name : ambient_removal type : AmbientRNARemoval params : raw_counts_path : /path/to/raw_feature_bc_matrix.h5 filtered_counts_path : /path/to/filtered_feature_bc_matrix.h5 ndims : 30 resolution : 0.8 - name : doublet_detection type : DoubletDetection params : expected_doublet_rate : 0.05 - name : filtering type : Filtering params : filters : n_genes_by_counts : type : numeric pct_counts_mt : type : numeric predicted_doublet : type : boolean - name : post_qc type : QCMetrics params : mito_pattern : \"^MT-\" ribo_pattern : \"^RP[SL]\" - name : normalization type : PearsonNormalization params : n_top_genes : 2000 - name : dim_reduction type : DimensionalityReduction params : n_pcs : 50 compute_umap : true compute_tsne : true - name : report_generator type : ReportGenerator params : generate_html : true Save this as complete_config.yaml and run it as before: python -m /workspace/scripts/run_pipeline.py --config complete_config.yaml","title":"Example with a Complete Analysis"},{"location":"getting-started/#resuming-from-a-checkpoint","text":"If your pipeline fails or stops for any reason, you can resume from the last checkpoint: python -m /workspace/scripts/run_pipeline.py --config complete_config.yaml --checkpoint doublet_detection This will resume the pipeline from after the \"doublet_detection\" module.","title":"Resuming from a Checkpoint"},{"location":"getting-started/#viewing-the-results","text":"After running the pipeline, check the output directory: output/ \u251c\u2500\u2500 checkpoints/ # Pipeline checkpoints \u251c\u2500\u2500 images/ # Generated figures \u251c\u2500\u2500 analysis_report.md # Markdown report \u2514\u2500\u2500 analysis_report.html # HTML report Open the HTML report in a web browser to view the analysis results, including visualizations and parameter settings.","title":"Viewing the Results"},{"location":"getting-started/#next-steps","text":"Explore the Configuration documentation to learn about all available options Browse the Modules section to understand each analysis step in detail Check out the Examples for more use cases and sample analyses","title":"Next Steps"},{"location":"installation/","text":"Installation \u00b6 This guide walks you through the installation process for Anatalyst. ALL OF THIS IS WIP AND WILL NEED TO BE UPDATED Installation Methods \u00b6 Method 1: Using docker (Recommended) \u00b6 # Pull the Docker image docker pull ghcr.io/yourusername/downstream-scrnaseq:latest Using devcontainer (For Development) \u00b6 If you're using Visual Studio Code, a devcontainer configuration is provided in the .devcontainer folder. This allows you to develop within a container that has all dependencies pre-installed. Install the Remote Development extension pack in VS Code Open the repository folder in VS Code When prompted, click \"Reopen in Container\" VS Code will build the container and provide you with a fully configured development environment Troubleshooting \u00b6 Common Issues \u00b6 R Bridge Connection Issues \u00b6 If the R bridge fails to connect: Error in R bridge: /bin/sh: 1: Rscript: not found Make sure R is installed and the Rscript executable is in your PATH. Memory Errors with Large Datasets \u00b6 If you encounter memory errors when processing large datasets: MemoryError : Unable to allocate array with shape ... Try increasing the memory limit in your configuration file: pipeline : r_memory_limit_gb : 16 # Increase this value For additional issues, please check the GitHub Issues (dead link for now) page or submit a new issue.","title":"Installation"},{"location":"installation/#installation","text":"This guide walks you through the installation process for Anatalyst. ALL OF THIS IS WIP AND WILL NEED TO BE UPDATED","title":"Installation"},{"location":"installation/#installation-methods","text":"","title":"Installation Methods"},{"location":"installation/#method-1-using-docker-recommended","text":"# Pull the Docker image docker pull ghcr.io/yourusername/downstream-scrnaseq:latest","title":"Method 1: Using docker (Recommended)"},{"location":"installation/#using-devcontainer-for-development","text":"If you're using Visual Studio Code, a devcontainer configuration is provided in the .devcontainer folder. This allows you to develop within a container that has all dependencies pre-installed. Install the Remote Development extension pack in VS Code Open the repository folder in VS Code When prompted, click \"Reopen in Container\" VS Code will build the container and provide you with a fully configured development environment","title":"Using devcontainer (For Development)"},{"location":"installation/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"installation/#common-issues","text":"","title":"Common Issues"},{"location":"installation/#r-bridge-connection-issues","text":"If the R bridge fails to connect: Error in R bridge: /bin/sh: 1: Rscript: not found Make sure R is installed and the Rscript executable is in your PATH.","title":"R Bridge Connection Issues"},{"location":"installation/#memory-errors-with-large-datasets","text":"If you encounter memory errors when processing large datasets: MemoryError : Unable to allocate array with shape ... Try increasing the memory limit in your configuration file: pipeline : r_memory_limit_gb : 16 # Increase this value For additional issues, please check the GitHub Issues (dead link for now) page or submit a new issue.","title":"Memory Errors with Large Datasets"},{"location":"about/","text":"TBD \u00b6","title":"About"},{"location":"about/#tbd","text":"","title":"TBD"},{"location":"api/","text":"TBD \u00b6","title":"Overview"},{"location":"api/#tbd","text":"","title":"TBD"},{"location":"api/core/","text":"TBD \u00b6","title":"Core Classes"},{"location":"api/core/#tbd","text":"","title":"TBD"},{"location":"api/utils/","text":"TBD \u00b6","title":"Utilities"},{"location":"api/utils/#tbd","text":"","title":"TBD"},{"location":"configuration/","text":"Configuration Overview \u00b6 Anatalyst uses YAML configuration files to define the pipeline structure and module parameters. This flexible approach allows you to customize the analysis workflow without changing code. Configuration File Structure \u00b6 A typical configuration file has two main sections: Pipeline Settings : Global settings for the entire pipeline Modules : List of modules to execute, with their parameters pipeline : name : my_analysis_pipeline output_dir : ./output r_memory_limit_gb : 8 figure_defaults : width : 8 height : 6 checkpointing : enabled : true modules_to_checkpoint : all max_checkpoints : 5 modules : - name : module1 type : ModuleType1 params : param1 : value1 param2 : value2 - name : module2 type : ModuleType2 params : param1 : value1 param2 : value2 Pipeline Settings \u00b6 The pipeline section contains global settings that apply to the entire pipeline: Setting Type Description name string Name of the pipeline (used in reports) output_dir string Directory where results will be saved r_memory_limit_gb number Memory limit for R processes (in GB) figure_defaults object Default settings for generated figures checkpointing object Configuration for checkpoint system Figure Defaults \u00b6 The figure_defaults section controls the default size and appearance of generated figures: figure_defaults : width : 8 # Width in inches height : 6 # Height in inches Checkpointing \u00b6 The checkpointing section configures the checkpoint system, which allows resuming a pipeline from intermediate points: checkpointing : enabled : true # Whether checkpointing is enabled modules_to_checkpoint : all # Which modules to create checkpoints for max_checkpoints : 5 # Maximum number of checkpoints to keep For modules_to_checkpoint , you can specify: - all : Checkpoint after every module (default) - A list of module names: ['data_loading', 'filtering'] Module Configuration \u00b6 Each entry in the modules list defines a module to execute in the pipeline: - name : module_name # A unique name for this module instance type : ModuleType # The type of module to use (class name) params : # Module-specific parameters param1 : value1 param2 : value2 Module Execution Order \u00b6 Modules are executed in the order they appear in the configuration file. Each module: Receives the data context from previous modules Applies its analysis steps Updates the data context for subsequent modules Module Parameters \u00b6 Each module has its own set of parameters, defined in the params section. Refer to the individual module documentation for details on available parameters. Validation \u00b6 The pipeline configuration is validated when loaded: Required sections ( pipeline and modules ) must be present The pipeline section must have a name and output_dir Each module must have a name and type Module parameters are validated against each module's parameter schema Example Configuration \u00b6 Here's a minimal example configuration: pipeline : name : minimal_pipeline output_dir : ./output modules : - name : data_loading type : DataLoading params : file_path : /path/to/data.h5 - name : qc_metrics type : QCMetrics params : mito_pattern : \"^MT-\" ribo_pattern : \"^RP[SL]\" - name : report_generator type : ReportGenerator For more complex examples, see the Examples page. Command Line Options \u00b6 When running the pipeline, you can specify additional options: python -m /workspace/scripts/run_pipeline.py --config my_config.yaml --checkpoint module_name --log-file pipeline.log Options: - --config : Path to the configuration file (required) - --checkpoint : Resume from a specific checkpoint - --log-file : Path to save log output - --log-level : Logging level (DEBUG, INFO, WARNING, ERROR) Next Steps \u00b6 Browse example configurations to see complete pipeline setups Check the module documentation for details on available modules and their parameters Learn how to create custom modules to extend the pipeline","title":"Overview"},{"location":"configuration/#configuration-overview","text":"Anatalyst uses YAML configuration files to define the pipeline structure and module parameters. This flexible approach allows you to customize the analysis workflow without changing code.","title":"Configuration Overview"},{"location":"configuration/#configuration-file-structure","text":"A typical configuration file has two main sections: Pipeline Settings : Global settings for the entire pipeline Modules : List of modules to execute, with their parameters pipeline : name : my_analysis_pipeline output_dir : ./output r_memory_limit_gb : 8 figure_defaults : width : 8 height : 6 checkpointing : enabled : true modules_to_checkpoint : all max_checkpoints : 5 modules : - name : module1 type : ModuleType1 params : param1 : value1 param2 : value2 - name : module2 type : ModuleType2 params : param1 : value1 param2 : value2","title":"Configuration File Structure"},{"location":"configuration/#pipeline-settings","text":"The pipeline section contains global settings that apply to the entire pipeline: Setting Type Description name string Name of the pipeline (used in reports) output_dir string Directory where results will be saved r_memory_limit_gb number Memory limit for R processes (in GB) figure_defaults object Default settings for generated figures checkpointing object Configuration for checkpoint system","title":"Pipeline Settings"},{"location":"configuration/#figure-defaults","text":"The figure_defaults section controls the default size and appearance of generated figures: figure_defaults : width : 8 # Width in inches height : 6 # Height in inches","title":"Figure Defaults"},{"location":"configuration/#checkpointing","text":"The checkpointing section configures the checkpoint system, which allows resuming a pipeline from intermediate points: checkpointing : enabled : true # Whether checkpointing is enabled modules_to_checkpoint : all # Which modules to create checkpoints for max_checkpoints : 5 # Maximum number of checkpoints to keep For modules_to_checkpoint , you can specify: - all : Checkpoint after every module (default) - A list of module names: ['data_loading', 'filtering']","title":"Checkpointing"},{"location":"configuration/#module-configuration","text":"Each entry in the modules list defines a module to execute in the pipeline: - name : module_name # A unique name for this module instance type : ModuleType # The type of module to use (class name) params : # Module-specific parameters param1 : value1 param2 : value2","title":"Module Configuration"},{"location":"configuration/#module-execution-order","text":"Modules are executed in the order they appear in the configuration file. Each module: Receives the data context from previous modules Applies its analysis steps Updates the data context for subsequent modules","title":"Module Execution Order"},{"location":"configuration/#module-parameters","text":"Each module has its own set of parameters, defined in the params section. Refer to the individual module documentation for details on available parameters.","title":"Module Parameters"},{"location":"configuration/#validation","text":"The pipeline configuration is validated when loaded: Required sections ( pipeline and modules ) must be present The pipeline section must have a name and output_dir Each module must have a name and type Module parameters are validated against each module's parameter schema","title":"Validation"},{"location":"configuration/#example-configuration","text":"Here's a minimal example configuration: pipeline : name : minimal_pipeline output_dir : ./output modules : - name : data_loading type : DataLoading params : file_path : /path/to/data.h5 - name : qc_metrics type : QCMetrics params : mito_pattern : \"^MT-\" ribo_pattern : \"^RP[SL]\" - name : report_generator type : ReportGenerator For more complex examples, see the Examples page.","title":"Example Configuration"},{"location":"configuration/#command-line-options","text":"When running the pipeline, you can specify additional options: python -m /workspace/scripts/run_pipeline.py --config my_config.yaml --checkpoint module_name --log-file pipeline.log Options: - --config : Path to the configuration file (required) - --checkpoint : Resume from a specific checkpoint - --log-file : Path to save log output - --log-level : Logging level (DEBUG, INFO, WARNING, ERROR)","title":"Command Line Options"},{"location":"configuration/#next-steps","text":"Browse example configurations to see complete pipeline setups Check the module documentation for details on available modules and their parameters Learn how to create custom modules to extend the pipeline","title":"Next Steps"},{"location":"configuration/examples/","text":"TBD \u00b6","title":"Examples"},{"location":"configuration/examples/#tbd","text":"","title":"TBD"},{"location":"customization/","text":"TBD \u00b6","title":"Overview"},{"location":"customization/#tbd","text":"","title":"TBD"},{"location":"customization/new-modules/","text":"Creating Custom Modules for the Single-Cell Pipeline \u00b6 Overview \u00b6 Anatalyst is designed to be extensible, allowing you to create custom modules that integrate seamlessly with the existing workflow or create an entirely new workflow. This guide will walk you through the process of creating a new analysis module. Module Structure \u00b6 Each module is a Python class that inherits from AnalysisModule . The basic structure includes: Initialization method Parameter schema run method Optional helper methods Step-by-Step Guide \u00b6 1. Create the Module File \u00b6 Create a new file in sc_pipeline/modules/ with a descriptive name, e.g., myanalysis.py . 2. Import Required Modules \u00b6 # At a minimum import logging from sc_pipeline.core.module import AnalysisModule 3. Define the Module Class \u00b6 class MyAnalysis ( AnalysisModule ): \"\"\" Description of your custom module's purpose. \"\"\" # Define the parameter schema PARAMETER_SCHEMA = { 'param1' : { 'type' : str , # Parameter type 'required' : True , # Whether the parameter is mandatory 'description' : 'Description of the parameter' }, 'optional_param' : { 'type' : int , 'default' : 10 , # Default value if not provided 'description' : 'An optional parameter' } } def __init__ ( self , name , params ): # Call the parent class constructor super () . __init__ ( name , params ) # Set up logging self . logger = logging . getLogger ( f \"Module. { name } \" ) # Define required inputs and outputs self . required_inputs = [ \"data\" ] # Inputs this module needs self . outputs = [ \"data\" ] # Outputs this module will produce 4. Implement the run Method \u00b6 def run ( self , data_context ): \"\"\" Main method to execute the module's analysis. Args: data_context: Shared data context containing pipeline data Returns: bool: True if successful, False otherwise \"\"\" try : # Retrieve the AnnData object adata = data_context . get ( \"data\" ) # Access module parameters param1 = self . params . get ( 'param1' ) optional_param = self . params . get ( 'optional_param' , 10 ) # Perform your analysis # Example: Do something with the data # Optional: Create visualization if self . params . get ( 'create_plots' , True ): self . _create_plots ( adata , data_context ) # Update the data context data_context . set ( \"data\" , adata ) return True except Exception as e : self . logger . error ( f \"Error in analysis: { e } \" , exc_info = True ) return False 5. Add Optional Visualization Method \u00b6 def _create_plots ( self , adata , data_context ): \"\"\" Create and save visualization figures. Args: adata: AnnData object data_context: Shared data context \"\"\" try : # Create a matplotlib figure fig , ax = plt . subplots ( figsize = ( 8 , 6 )) # Create your plot # sc.pl.something(adata, ax=ax) # Save the figure using the parent class method img_path = self . save_figure ( data_context , self . name , fig ) # Add figure to the report data_context . add_figure ( module_name = self . name , title = \"My Analysis Plot\" , description = \"Description of the plot\" , image_path = img_path ) except Exception as e : self . logger . warning ( f \"Error creating plots: { e } \" ) Module Best Practices \u00b6 Use logging for tracking module progress and errors Handle exceptions gracefully Provide clear documentation and parameter descriptions Create visualizations when possible Minimize data transformation, preferring to add information to the AnnData object Using Your Custom Module \u00b6 To use your new module in a pipeline configuration: modules : - name : my_custom_analysis type : MyAnalysis params : param1 : \"example_value\" optional_param : 20 Notes \u00b6 Modules should be idempotent (can be run multiple times without side effects) Prefer adding information to AnnData's .obs , .var , .uns , or as layers Keep modules focused on a single type of analysis","title":"Creating New Modules"},{"location":"customization/new-modules/#creating-custom-modules-for-the-single-cell-pipeline","text":"","title":"Creating Custom Modules for the Single-Cell Pipeline"},{"location":"customization/new-modules/#overview","text":"Anatalyst is designed to be extensible, allowing you to create custom modules that integrate seamlessly with the existing workflow or create an entirely new workflow. This guide will walk you through the process of creating a new analysis module.","title":"Overview"},{"location":"customization/new-modules/#module-structure","text":"Each module is a Python class that inherits from AnalysisModule . The basic structure includes: Initialization method Parameter schema run method Optional helper methods","title":"Module Structure"},{"location":"customization/new-modules/#step-by-step-guide","text":"","title":"Step-by-Step Guide"},{"location":"customization/new-modules/#1-create-the-module-file","text":"Create a new file in sc_pipeline/modules/ with a descriptive name, e.g., myanalysis.py .","title":"1. Create the Module File"},{"location":"customization/new-modules/#2-import-required-modules","text":"# At a minimum import logging from sc_pipeline.core.module import AnalysisModule","title":"2. Import Required Modules"},{"location":"customization/new-modules/#3-define-the-module-class","text":"class MyAnalysis ( AnalysisModule ): \"\"\" Description of your custom module's purpose. \"\"\" # Define the parameter schema PARAMETER_SCHEMA = { 'param1' : { 'type' : str , # Parameter type 'required' : True , # Whether the parameter is mandatory 'description' : 'Description of the parameter' }, 'optional_param' : { 'type' : int , 'default' : 10 , # Default value if not provided 'description' : 'An optional parameter' } } def __init__ ( self , name , params ): # Call the parent class constructor super () . __init__ ( name , params ) # Set up logging self . logger = logging . getLogger ( f \"Module. { name } \" ) # Define required inputs and outputs self . required_inputs = [ \"data\" ] # Inputs this module needs self . outputs = [ \"data\" ] # Outputs this module will produce","title":"3. Define the Module Class"},{"location":"customization/new-modules/#4-implement-the-run-method","text":"def run ( self , data_context ): \"\"\" Main method to execute the module's analysis. Args: data_context: Shared data context containing pipeline data Returns: bool: True if successful, False otherwise \"\"\" try : # Retrieve the AnnData object adata = data_context . get ( \"data\" ) # Access module parameters param1 = self . params . get ( 'param1' ) optional_param = self . params . get ( 'optional_param' , 10 ) # Perform your analysis # Example: Do something with the data # Optional: Create visualization if self . params . get ( 'create_plots' , True ): self . _create_plots ( adata , data_context ) # Update the data context data_context . set ( \"data\" , adata ) return True except Exception as e : self . logger . error ( f \"Error in analysis: { e } \" , exc_info = True ) return False","title":"4. Implement the run Method"},{"location":"customization/new-modules/#5-add-optional-visualization-method","text":"def _create_plots ( self , adata , data_context ): \"\"\" Create and save visualization figures. Args: adata: AnnData object data_context: Shared data context \"\"\" try : # Create a matplotlib figure fig , ax = plt . subplots ( figsize = ( 8 , 6 )) # Create your plot # sc.pl.something(adata, ax=ax) # Save the figure using the parent class method img_path = self . save_figure ( data_context , self . name , fig ) # Add figure to the report data_context . add_figure ( module_name = self . name , title = \"My Analysis Plot\" , description = \"Description of the plot\" , image_path = img_path ) except Exception as e : self . logger . warning ( f \"Error creating plots: { e } \" )","title":"5. Add Optional Visualization Method"},{"location":"customization/new-modules/#module-best-practices","text":"Use logging for tracking module progress and errors Handle exceptions gracefully Provide clear documentation and parameter descriptions Create visualizations when possible Minimize data transformation, preferring to add information to the AnnData object","title":"Module Best Practices"},{"location":"customization/new-modules/#using-your-custom-module","text":"To use your new module in a pipeline configuration: modules : - name : my_custom_analysis type : MyAnalysis params : param1 : \"example_value\" optional_param : 20","title":"Using Your Custom Module"},{"location":"customization/new-modules/#notes","text":"Modules should be idempotent (can be run multiple times without side effects) Prefer adding information to AnnData's .obs , .var , .uns , or as layers Keep modules focused on a single type of analysis","title":"Notes"},{"location":"examples/","text":"TBD \u00b6","title":"Overview"},{"location":"examples/#tbd","text":"","title":"TBD"},{"location":"examples/pbmc/","text":"TBD \u00b6","title":"PBMC Analysis"},{"location":"examples/pbmc/#tbd","text":"","title":"TBD"},{"location":"modules/","text":"Module Dependencies \u00b6 Some modules depend on the output of other modules. View the details of each module's indivual page to make sure they are implemented in an order that makes sense! Module Interface \u00b6 All modules inherit from the AnalysisModule base class and implement the following key methods: __init__(name, params) : Initializes the module with a name and parameters run(data_context) : Executes the module's functionality on the data context validate_inputs(data_context) : Ensures all required inputs are available validate_outputs(data_context) : Ensures all expected outputs were created Creating Custom Modules \u00b6 You can create custom modules by subclassing AnalysisModule . See the Creating New Modules guide for detailed instructions. Configuring Modules \u00b6 Modules are configured through the pipeline configuration file using YAML syntax. All modules have a name and type. The name is customizable and designed to make logs and checkpoints clear to the user. The type is case-insensitive name of the module file; for example, DataLoading maps to modules/dataloading.py . Additionally, Each module has its own set of parameters, detailed in the individual module documentation. Example configuration for a module: - name : filtering # Name that appears in logs and checkpoints type : Filtering # maps to modules/filtering.py params : # Parameters accepted by filtering module filters : n_genes_by_counts : type : numeric min : 200 max : 6000 pct_counts_mt : type : numeric max : 20 create_plots : true Visit the individual module pages for detailed documentation on each module's parameters and functionality.","title":"Overview"},{"location":"modules/#module-dependencies","text":"Some modules depend on the output of other modules. View the details of each module's indivual page to make sure they are implemented in an order that makes sense!","title":"Module Dependencies"},{"location":"modules/#module-interface","text":"All modules inherit from the AnalysisModule base class and implement the following key methods: __init__(name, params) : Initializes the module with a name and parameters run(data_context) : Executes the module's functionality on the data context validate_inputs(data_context) : Ensures all required inputs are available validate_outputs(data_context) : Ensures all expected outputs were created","title":"Module Interface"},{"location":"modules/#creating-custom-modules","text":"You can create custom modules by subclassing AnalysisModule . See the Creating New Modules guide for detailed instructions.","title":"Creating Custom Modules"},{"location":"modules/#configuring-modules","text":"Modules are configured through the pipeline configuration file using YAML syntax. All modules have a name and type. The name is customizable and designed to make logs and checkpoints clear to the user. The type is case-insensitive name of the module file; for example, DataLoading maps to modules/dataloading.py . Additionally, Each module has its own set of parameters, detailed in the individual module documentation. Example configuration for a module: - name : filtering # Name that appears in logs and checkpoints type : Filtering # maps to modules/filtering.py params : # Parameters accepted by filtering module filters : n_genes_by_counts : type : numeric min : 200 max : 6000 pct_counts_mt : type : numeric max : 20 create_plots : true Visit the individual module pages for detailed documentation on each module's parameters and functionality.","title":"Configuring Modules"},{"location":"modules/ambientrnaremoval/","text":"TBD \u00b6","title":"AmbientRNARemoval"},{"location":"modules/ambientrnaremoval/#tbd","text":"","title":"TBD"},{"location":"modules/dataloading/","text":"TBD \u00b6","title":"DataLoading"},{"location":"modules/dataloading/#tbd","text":"","title":"TBD"},{"location":"modules/dimensionalityreduction/","text":"TBD \u00b6","title":"DimensionalityReduction"},{"location":"modules/dimensionalityreduction/#tbd","text":"","title":"TBD"},{"location":"modules/doubletdetection/","text":"TBD \u00b6","title":"DoubletDetection"},{"location":"modules/doubletdetection/#tbd","text":"","title":"TBD"},{"location":"modules/filtering/","text":"TBD \u00b6","title":"Filtering"},{"location":"modules/filtering/#tbd","text":"","title":"TBD"},{"location":"modules/pearsonnormalization/","text":"TBD \u00b6","title":"PearsonNormalization"},{"location":"modules/pearsonnormalization/#tbd","text":"","title":"TBD"},{"location":"modules/qcmetrics/","text":"QCMetrics Module \u00b6 The QCMetrics module calculates quality control metrics for single-cell RNA-seq data, including gene and UMI counts per cell, as well as the percentage of reads mapping to mitochondrial and ribosomal genes. Overview \u00b6 Quality control (QC) is a critical step in single-cell RNA-seq analysis. This module uses Scanpy's pp.calculate_qc_metrics function to compute standard QC metrics and, optionally, generates visualizations of the distributions. Parameters \u00b6 Parameter Type Default Description mito_pattern string ^MT- Regex pattern to identify mitochondrial genes ribo_pattern string ^RP[SL] Regex pattern to identify ribosomal genes create_plots boolean True Whether to generate plots of QC metrics plot_title string None Title for generated plots layer_key string None If provided, calculate metrics on this layer instead of .X Example Configuration \u00b6 - name : pre_qc type : QCMetrics params : mito_pattern : \"^MT-\" ribo_pattern : \"^RP[SL]\" create_plots : true plot_title : \"Pre-filtering QC Metrics\" Input/Output \u00b6 Inputs \u00b6 data : An AnnData object containing single-cell data Outputs \u00b6 Updates the data object with: .obs['n_genes_by_counts'] : Number of genes with positive counts in each cell .obs['total_counts'] : Total counts per cell .obs['pct_counts_mt'] : Percentage of counts in mitochondrial genes .obs['pct_counts_ribo'] : Percentage of counts in ribosomal genes Visualization of these metrics in the report (if create_plots is True ) Functionality \u00b6 The module performs the following operations: Calculates basic QC metrics (genes and UMIs per cell) Identifies mitochondrial genes based on the provided pattern Calculates the percentage of reads mapping to mitochondrial genes per cell Identifies ribosomal genes based on the provided pattern Calculates the percentage of reads mapping to ribosomal genes per cell Optionally generates violin plots showing the distribution of these metrics Visualization \u00b6 When create_plots is set to True , the module generates violin plots for: Number of genes detected per cell Total UMI counts per cell Percentage of counts from mitochondrial genes Percentage of counts from ribosomal genes These plots are saved as images and included in the final report. Usage Notes \u00b6 This module may be run twice in a pipeline: once before filtering to identify outliers and again after filtering to confirm the effectiveness of filtering steps. Different organisms may require adjusting the mito_pattern and ribo_pattern . For example: Human: ^MT- (mitochondrial), ^RP[SL] (ribosomal) Mouse: ^mt- (mitochondrial), ^Rp[sl] (ribosomal) High mitochondrial percentage often indicates cell stress or apoptosis High ribosomal percentage may indicate cell stress or high protein synthesis activity Implementation Details \u00b6 The module uses Scanpy's pp.calculate_qc_metrics function, which computes: Number of genes detected per cell Total counts per cell Percent of counts in feature subsets (e.g., mitochondrial genes) The module's visualization uses Scanpy's pl.violin functionality for creating the multi-panel violin plots. See Also \u00b6 Filtering Module : For filtering cells based on QC metrics Scanpy QC Documentation : For more details on the underlying implementation API Reference \u00b6 ::: sc_pipeline.modules.qcmetrics.QCMetrics options: show_root_heading: true show_source: true","title":"QCMetrics"},{"location":"modules/qcmetrics/#qcmetrics-module","text":"The QCMetrics module calculates quality control metrics for single-cell RNA-seq data, including gene and UMI counts per cell, as well as the percentage of reads mapping to mitochondrial and ribosomal genes.","title":"QCMetrics Module"},{"location":"modules/qcmetrics/#overview","text":"Quality control (QC) is a critical step in single-cell RNA-seq analysis. This module uses Scanpy's pp.calculate_qc_metrics function to compute standard QC metrics and, optionally, generates visualizations of the distributions.","title":"Overview"},{"location":"modules/qcmetrics/#parameters","text":"Parameter Type Default Description mito_pattern string ^MT- Regex pattern to identify mitochondrial genes ribo_pattern string ^RP[SL] Regex pattern to identify ribosomal genes create_plots boolean True Whether to generate plots of QC metrics plot_title string None Title for generated plots layer_key string None If provided, calculate metrics on this layer instead of .X","title":"Parameters"},{"location":"modules/qcmetrics/#example-configuration","text":"- name : pre_qc type : QCMetrics params : mito_pattern : \"^MT-\" ribo_pattern : \"^RP[SL]\" create_plots : true plot_title : \"Pre-filtering QC Metrics\"","title":"Example Configuration"},{"location":"modules/qcmetrics/#inputoutput","text":"","title":"Input/Output"},{"location":"modules/qcmetrics/#inputs","text":"data : An AnnData object containing single-cell data","title":"Inputs"},{"location":"modules/qcmetrics/#outputs","text":"Updates the data object with: .obs['n_genes_by_counts'] : Number of genes with positive counts in each cell .obs['total_counts'] : Total counts per cell .obs['pct_counts_mt'] : Percentage of counts in mitochondrial genes .obs['pct_counts_ribo'] : Percentage of counts in ribosomal genes Visualization of these metrics in the report (if create_plots is True )","title":"Outputs"},{"location":"modules/qcmetrics/#functionality","text":"The module performs the following operations: Calculates basic QC metrics (genes and UMIs per cell) Identifies mitochondrial genes based on the provided pattern Calculates the percentage of reads mapping to mitochondrial genes per cell Identifies ribosomal genes based on the provided pattern Calculates the percentage of reads mapping to ribosomal genes per cell Optionally generates violin plots showing the distribution of these metrics","title":"Functionality"},{"location":"modules/qcmetrics/#visualization","text":"When create_plots is set to True , the module generates violin plots for: Number of genes detected per cell Total UMI counts per cell Percentage of counts from mitochondrial genes Percentage of counts from ribosomal genes These plots are saved as images and included in the final report.","title":"Visualization"},{"location":"modules/qcmetrics/#usage-notes","text":"This module may be run twice in a pipeline: once before filtering to identify outliers and again after filtering to confirm the effectiveness of filtering steps. Different organisms may require adjusting the mito_pattern and ribo_pattern . For example: Human: ^MT- (mitochondrial), ^RP[SL] (ribosomal) Mouse: ^mt- (mitochondrial), ^Rp[sl] (ribosomal) High mitochondrial percentage often indicates cell stress or apoptosis High ribosomal percentage may indicate cell stress or high protein synthesis activity","title":"Usage Notes"},{"location":"modules/qcmetrics/#implementation-details","text":"The module uses Scanpy's pp.calculate_qc_metrics function, which computes: Number of genes detected per cell Total counts per cell Percent of counts in feature subsets (e.g., mitochondrial genes) The module's visualization uses Scanpy's pl.violin functionality for creating the multi-panel violin plots.","title":"Implementation Details"},{"location":"modules/qcmetrics/#see-also","text":"Filtering Module : For filtering cells based on QC metrics Scanpy QC Documentation : For more details on the underlying implementation","title":"See Also"},{"location":"modules/qcmetrics/#api-reference","text":"::: sc_pipeline.modules.qcmetrics.QCMetrics options: show_root_heading: true show_source: true","title":"API Reference"},{"location":"modules/reportgenerator/","text":"TBD \u00b6","title":"ReportGenerator"},{"location":"modules/reportgenerator/#tbd","text":"","title":"TBD"}]}