{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Anatalyst: The Analysis Catalyst - A Modular Pipeline built for Single-cell RNA-seq Analysis","text":"<p>Anatalyst is a flexible, modular Python pipeline designed to facilitate boilerplate analytical workflows that leverage existing Python and R programs. The current scope of module support is focused on Single Cell RNA sequencing, built on top of Scanpy, providing a customizable workflow for common single-cell analysis tasks.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Modular Architecture: Each analysis step is encapsulated in a module that can be included or excluded as needed</li> <li>Configurable Pipeline: Simple YAML configuration to customize pipeline parameters</li> <li>Checkpoint System: Save and resume pipeline execution from checkpoints</li> <li>R Integration: Seamless integration of R tools (like SoupX) for specialized analyses</li> <li>Reproducible Analysis: Detailed reports with visualizations and parameter settings</li> <li>Framework Flexibility: New modules can be custom-built and inserted into the pipeline to allow for any type of sequential analysis</li> </ul>"},{"location":"#workflow-overview","title":"Workflow Overview","text":"<p>Anatalyst provides a comprehensive workflow for single-cell analysis:</p> <ol> <li>Data Loading: Import aligned 10X single-cell data using an .h5 file</li> <li>Quality Control: Calculate QC metrics and visualize distributions</li> <li>Ambient RNA Removal: Remove background RNA contamination using SoupX</li> <li>Doublet Detection: Identify and flag potential cell doublets</li> <li>Cell Filtering: Filter out low-quality cells and outliers</li> <li>Pearson Normalization: Normalize data using Pearson residuals</li> <li>Dimensionality Reduction: PCA, UMAP, and t-SNE for visualization and analysis</li> <li>Report Generation: Create comprehensive HTML reports with key figures</li> </ol>"},{"location":"#quick-start","title":"Quick Start","text":"<p>TBD - expecting pull Docker container with pipeline already installed Mount local directory for input/output and extra module insertion?</p> <pre><code># Install the container\ndocker pull something-or-other\n\n# Run a pipeline with a configuration file\npython -m sc_pipeline.scripts.run_pipeline --config my_config.yaml\n# This command will likely change and may just be the way the container is launched? \n# Perhaps these args just get passed to docker compose via ENV variables and we make a new entrypoint.sh file?\n</code></pre> <p>Check out the Getting Started guide for more detailed instructions.</p>"},{"location":"#pipeline-diagram","title":"Pipeline Diagram","text":"<pre><code>insert diagram here\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will walk you through running your first analysis with Anatalyst.</p>"},{"location":"getting-started/#basic-usage","title":"Basic Usage","text":"<p>Anatalyst is designed to be run from a configuration file that defines the modules to use and their parameters. The basic workflow is:</p> <ol> <li>Create a configuration file</li> <li>Run the pipeline</li> <li>Review the results</li> </ol>"},{"location":"getting-started/#creating-a-configuration-file","title":"Creating a Configuration File","text":"<p>Create a YAML file that defines your pipeline. Here's a minimal example:</p> <pre><code>pipeline:\n  name: my_first_pipeline\n  output_dir: ./output\n  r_memory_limit_gb: 8\n  figure_defaults:\n    width: 8\n    height: 6\n\nmodules:\n  - name: data_loading\n    type: DataLoading\n    params:\n      file_path: /path/to/your/filtered_feature_bc_matrix.h5\n\n  - name: qc_metrics\n    type: QCMetrics\n    params:\n      mito_pattern: \"^MT-\"\n      ribo_pattern: \"^RP[SL]\"\n\n  - name: report_generator\n    type: ReportGenerator\n</code></pre> <p>Save this as <code>minimal_config.yaml</code>.</p>"},{"location":"getting-started/#running-the-pipeline","title":"Running the Pipeline","text":"<p>Execute the pipeline using the <code>run_pipeline.py</code> script:</p> <pre><code>python -m /workspace/scripts/run_pipeline.py --config minimal_config.yaml\n</code></pre> <p>This will:</p> <ol> <li>Load your data</li> <li>Calculate QC metrics</li> <li>Generate a report in the output directory</li> </ol>"},{"location":"getting-started/#example-with-a-complete-analysis","title":"Example with a Complete Analysis","text":"<p>Here's a more comprehensive example configuration for a full analysis workflow:</p> <pre><code>pipeline:\n  name: complete_analysis\n  output_dir: ./output\n  r_memory_limit_gb: 8\n  figure_defaults:\n    width: 8\n    height: 6\n  checkpointing:\n    enabled: true\n    modules_to_checkpoint: all\n    max_checkpoints: 5\n\nmodules:\n  - name: data_loading\n    type: DataLoading\n    params:\n      file_path: /path/to/filtered_feature_bc_matrix.h5\n\n  - name: pre_qc\n    type: QCMetrics\n    params:\n      mito_pattern: \"^MT-\"\n      ribo_pattern: \"^RP[SL]\"\n      create_plots: true\n\n  - name: ambient_removal\n    type: AmbientRNARemoval\n    params:\n      raw_counts_path: /path/to/raw_feature_bc_matrix.h5\n      filtered_counts_path: /path/to/filtered_feature_bc_matrix.h5\n      ndims: 30\n      resolution: 0.8\n\n  - name: doublet_detection\n    type: DoubletDetection\n    params:\n      expected_doublet_rate: 0.05\n\n  - name: filtering\n    type: Filtering\n    params:\n      filters:\n        n_genes_by_counts:\n          type: numeric\n        pct_counts_mt:\n          type: numeric\n        predicted_doublet:\n          type: boolean\n\n  - name: post_qc\n    type: QCMetrics\n    params:\n      mito_pattern: \"^MT-\"\n      ribo_pattern: \"^RP[SL]\"\n\n  - name: normalization\n    type: PearsonNormalization\n    params:\n      n_top_genes: 2000\n\n  - name: dim_reduction\n    type: DimensionalityReduction\n    params:\n      n_pcs: 50\n      compute_umap: true\n      compute_tsne: true\n\n  - name: report_generator\n    type: ReportGenerator\n    params:\n      generate_html: true\n</code></pre> <p>Save this as <code>complete_config.yaml</code> and run it as before:</p> <pre><code>python -m /workspace/scripts/run_pipeline.py --config complete_config.yaml\n</code></pre>"},{"location":"getting-started/#resuming-from-a-checkpoint","title":"Resuming from a Checkpoint","text":"<p>If your pipeline fails or stops for any reason, you can resume from the last checkpoint:</p> <pre><code>python -m /workspace/scripts/run_pipeline.py --config complete_config.yaml --checkpoint doublet_detection\n</code></pre> <p>This will resume the pipeline from after the \"doublet_detection\" module.</p>"},{"location":"getting-started/#viewing-the-results","title":"Viewing the Results","text":"<p>After running the pipeline, check the output directory:</p> <pre><code>output/\n\u251c\u2500\u2500 checkpoints/          # Pipeline checkpoints\n\u251c\u2500\u2500 images/               # Generated figures\n\u251c\u2500\u2500 analysis_report.md    # Markdown report\n\u2514\u2500\u2500 analysis_report.html  # HTML report \n</code></pre> <p>Open the HTML report in a web browser to view the analysis results, including visualizations and parameter settings.</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the Configuration documentation to learn about all available options</li> <li>Browse the Modules section to understand each analysis step in detail</li> <li>Check out the Examples for more use cases and sample analyses</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>This guide walks you through the installation process for Anatalyst. ALL OF THIS IS WIP AND WILL NEED TO BE UPDATED</p>"},{"location":"installation/#installation-methods","title":"Installation Methods","text":""},{"location":"installation/#method-1-using-docker-recommended","title":"Method 1: Using docker (Recommended)","text":"<pre><code># Pull the Docker image\ndocker pull ghcr.io/yourusername/downstream-scrnaseq:latest\n</code></pre>"},{"location":"installation/#using-devcontainer-for-development","title":"Using devcontainer (For Development)","text":"<p>If you're using Visual Studio Code, a devcontainer configuration is provided in the <code>.devcontainer</code> folder. This allows you to develop within a container that has all dependencies pre-installed.</p> <ol> <li>Install the Remote Development extension pack in VS Code</li> <li>Open the repository folder in VS Code</li> <li>When prompted, click \"Reopen in Container\"</li> <li>VS Code will build the container and provide you with a fully configured development environment</li> </ol>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#common-issues","title":"Common Issues","text":""},{"location":"installation/#r-bridge-connection-issues","title":"R Bridge Connection Issues","text":"<p>If the R bridge fails to connect:</p> <pre><code>Error in R bridge: /bin/sh: 1: Rscript: not found\n</code></pre> <p>Make sure R is installed and the <code>Rscript</code> executable is in your PATH.</p>"},{"location":"installation/#memory-errors-with-large-datasets","title":"Memory Errors with Large Datasets","text":"<p>If you encounter memory errors when processing large datasets:</p> <pre><code>MemoryError: Unable to allocate array with shape...\n</code></pre> <p>Try increasing the memory limit in your configuration file:</p> <pre><code>pipeline:\n  r_memory_limit_gb: 16  # Increase this value\n</code></pre> <p>For additional issues, please check the GitHub Issues (dead link for now) page or submit a new issue.</p>"},{"location":"about/","title":"TBD","text":""},{"location":"api/","title":"TBD","text":""},{"location":"api/core/","title":"Anatalyst Pipeline Core API","text":"<p>This document provides detailed documentation for the core components of the <code>Anatalyst</code> framework. The core modules form the foundation of the pipeline system, handling tasks such as configuration parsing, data management, module execution, and pipeline orchestration.</p>"},{"location":"api/core/#overview","title":"Overview","text":"<p>Analyst is designed around a modular architecture that allows for flexible composition of analysis workflows. Each analysis is broken down into a series of modules that each perform a specific task with the data. The core components facilitate this by:</p> <ol> <li>Configuration Management: Reading and validating YAML configuration files that define pipeline steps</li> <li>Data Context: Managing shared data between modules and providing checkpointing capabilities</li> <li>Module System: Defining the base interface for all analysis modules</li> <li>Pipeline Execution: Orchestrating the execution of modules in sequence</li> </ol>"},{"location":"api/core/#core-components","title":"Core Components","text":""},{"location":"api/core/#configparser","title":"ConfigParser","text":"<p>The <code>ConfigParser</code> class handles the reading and validation of YAML configuration files that define pipeline structure and parameters.</p> <pre><code>from sc_pipeline.core.config import ConfigParser\n\nparser = ConfigParser()\nconfig = parser.parse(\"path/to/config.yaml\")\n</code></pre>"},{"location":"api/core/#key-methods","title":"Key Methods","text":"<ul> <li><code>parse(config_file)</code>: Parses a YAML configuration file and returns the validated configuration dictionary.</li> <li><code>_validate_config(config)</code>: Internal method that validates the structure of the configuration.</li> <li><code>_set_defaults(config)</code>: Internal method that sets default values for optional configuration parameters.</li> </ul>"},{"location":"api/core/#configuration-structure","title":"Configuration Structure","text":"<p>A basic configuration file should have the following structure:</p> <pre><code>pipeline:\n  name: my_pipeline\n  output_dir: ./output\n  r_memory_limit_gb: 8  # Optional\n  figure_defaults:      # Optional\n    width: 8\n    height: 6\n  checkpointing:        # Optional\n    enabled: true\n    modules_to_checkpoint: all  # or a list of module names\n    max_checkpoints: 5\n\nmodules:\n  - name: module1\n    type: ModuleType\n    params:\n      param1: value1\n      param2: value2\n\n  - name: module2\n    type: AnotherModuleType\n    params:\n      param1: value1\n</code></pre>"},{"location":"api/core/#scope","title":"Scope","text":"<p>In order to provide module level access to the pipeline's configuration, the executor adds the entire parsed configuration to the <code>DataContext</code> shortly after its instantiation before any modules are run. This can be accessed from inside any module using <code>data_context.get('CONFIG')</code></p>"},{"location":"api/core/#datacontext","title":"DataContext","text":"<p>The <code>DataContext</code> class provides a shared storage space for data passed between modules in the pipeline. It also offers checkpointing capabilities to save and restore pipeline state. Anything handled inside of a module that is not explicitly written to a file space or to the <code>DataContext</code> will be unavailable</p> <pre><code>from sc_pipeline.core.data_context import DataContext\n\n# Create a data context with checkpointing enabled\ndata_context = DataContext(checkpoint_dir=\"./checkpoints\", max_checkpoints=3)\n\n# Store and retrieve data\ncontext.set(\"key\", value)\nvalue = context.get(\"key\")\n\n# Save and load checkpoints\ncontext.save_checkpoint(\"after_module1\")\ncontext.load_checkpoint(\"after_module1\")\n</code></pre>"},{"location":"api/core/#figure-generation","title":"Figure Generation","text":"<p>To better facilitate record keeping and report generation at the end of the workflow, the <code>DataContext</code> class has a built in <code>add_figure</code> method. This takes the name of the module generating the figure, and optional title, description, caption, and image_path arguments. This is designed to be compatible with the <code>ReportGenerator</code> module, which will essentially string together an html document of these elements. The <code>save_figure</code> method of the <code>AnalysisModule</code> class returns the path of the saved image, which can then be passed right to the <code>DataContext.add_figure()</code> method if you want to include the generated figure. <pre><code>class MyModule(AnalysisModule):\n  def run(self, data_context: DataContext):\n    title = \"My Figure\"\n    desc = \"A description of my figure\"\n    caption = \"A caption for my figure\"\n\n    fig = sc.pl.embedding( # Returns matplot figure (from scanpy)\n              adata, \n              basis=pca_key, \n              color=None, \n              return_fig=True, \n              title=f\"My Figure\"\n            )\n    img_path = self.save_figure(data_context, self.name, fig)\n    data_context.add_figure(\n        module_name= self.name,\n        title= title,\n        description= desc,\n        caption=caption,\n        image_path= img_path\n    )\n    return\n</code></pre></p>"},{"location":"api/core/#key-methods_1","title":"Key Methods","text":"<ul> <li><code>set(key, value)</code>: Store data by key</li> <li><code>get(key, default=None)</code>: Retrieve data by key, with an optional default value</li> <li><code>__contains__(key)</code>: Check if a key exists (<code>key in context</code>)</li> <li><code>keys()</code>: Get all available data keys</li> <li><code>save_checkpoint(checkpoint_name)</code>: Save the current state to a checkpoint file (.pkl file of the DataContext itself)</li> <li><code>load_checkpoint(checkpoint_name)</code>: Load data from a checkpoint file</li> <li><code>add_figure(module_name, title=None, description=None, image_path=None, caption=None)</code>: Add a figure to be included in reports</li> </ul>"},{"location":"api/core/#analysismodule","title":"AnalysisModule","text":"<p>The <code>AnalysisModule</code> class is the base class for all analysis modules in the pipeline. It defines the interface that modules must implement and provides common functionality.</p> <pre><code>from sc_pipeline.core.module import AnalysisModule\n\nclass MyModule(AnalysisModule):\n    \"\"\"Custom module implementation.\"\"\"\n\n    PARAMETER_SCHEMA = {\n        'param1': {\n            'type': str,\n            'required': True,\n            'description': 'Description of parameter 1'\n        },\n        'param2': {\n            'type': int,\n            'default': 10,\n            'description': 'Description of parameter 2'\n        }\n    }\n\n    def __init__(self, name, params):\n        super().__init__(name, params)\n        self.required_inputs = [\"input1\", \"input2\"]\n        self.outputs = [\"output1\"]\n\n    def run(self, data_context):\n        # Implementation of module functionality\n        # Access parameters with self.params.get('param_name', default_value)\n        # Access inputs with data_context.get('input_name')\n        # Store outputs with data_context.set('output_name', value)\n        return True  # Return True if successful, False otherwise\n</code></pre>"},{"location":"api/core/#key-methods_2","title":"Key Methods","text":"<ul> <li><code>__init__(name, params)</code>: Initialize the module with a name and parameters</li> <li><code>run(data_context)</code>: Execute the module's analysis (must be implemented by subclasses)</li> <li><code>validate_inputs(data_context)</code>: Check if all required inputs are available</li> <li><code>validate_outputs(data_context)</code>: Check if all expected outputs were created</li> <li><code>_validate_params(provided_params)</code>: Validate parameters against schema and apply defaults</li> <li><code>get_metadata()</code>: Return metadata about the module</li> <li><code>save_figure(data_context, module_name, fig, figsize=None, name=None, output_dir=None, dpi=300)</code>: Save a matplotlib figure</li> </ul>"},{"location":"api/core/#parameter-schema-definition","title":"Parameter Schema Definition","text":"<p>Module parameters can be defined using the <code>PARAMETER_SCHEMA</code> class variable. This schema is used to validate parameters and apply defaults. Each parameter entry should include:</p> <ul> <li><code>type</code>: The expected Python type (e.g., <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code>, <code>list</code>, <code>dict</code>)</li> <li><code>required</code>: Whether the parameter is required (default: <code>False</code>)</li> <li><code>default</code>: Default value if not provided (optional)</li> <li><code>description</code>: Human-readable description of the parameter (optional)</li> </ul> <p>For list parameters, you can specify the expected element type using <code>element_type</code>.</p>"},{"location":"api/core/#pipelineexecutor","title":"PipelineExecutor","text":"<p>The <code>PipelineExecutor</code> class handles the orchestration of pipeline execution, running modules in sequence according to the configuration.</p> <pre><code>from sc_pipeline.core.executor import PipelineExecutor\n\n# Create and run a pipeline from a configuration file\nexecutor = PipelineExecutor(\"path/to/config.yaml\")\nsuccess = executor.run()\n\n# Optionally, start from a checkpoint\nsuccess = executor.run(start_from=\"after_module1\")\n</code></pre>"},{"location":"api/core/#key-methods_3","title":"Key Methods","text":"<ul> <li><code>__init__(config_file)</code>: Initialize the executor with a configuration file</li> <li><code>register_module_type(module_type, module_class)</code>: Register a module type with its implementing class</li> <li><code>run(start_from=None)</code>: Execute the pipeline, optionally starting from a checkpoint</li> <li><code>_get_module_class(module_type)</code>: Internal method to get the class for a module type</li> <li><code>_set_global_settings()</code>: Internal method to store global pipeline settings</li> </ul>"},{"location":"api/core/#utility-modules","title":"Utility Modules","text":""},{"location":"api/core/#rbridge","title":"RBridge","text":"<p>The <code>RBridge</code> class provides a bridge for calling R functions from Python, using a temporary workspace directory for file exchange.</p> <pre><code>from sc_pipeline.utils.r_bridge import RBridge\n\n# Create an R bridge with a specified memory limit\nr_bridge = RBridge(r_script_dir=\"./r_scripts\", memory_limit_gb=16)\n\n# Run an R script with arguments\nsuccess, stdout, stderr = r_bridge.run_r_script(\"script.R\", {\"arg1\": \"value1\", \"arg2\": \"value2\"})\n\n# Get the path to a file in the workspace\nfile_path = r_bridge.get_workspace_path(\"output.txt\")\n\n# Clean up when done\nr_bridge.cleanup_workspace()\n</code></pre>"},{"location":"api/core/#anndata-utilities","title":"AnnData Utilities","text":"<p>The <code>adata_utils</code> module provides utilities for working with AnnData objects, particularly focused on managing layers.</p> <pre><code>from sc_pipeline.utils.adata_utils import save_layer, set_active_layer\n\n# Save data as a layer in AnnData\nadata = save_layer(adata, name=\"raw_counts\", data=None, make_active=False)\n\n# Set a specific layer as the active layer (X matrix)\nadata = set_active_layer(adata, layer_name=\"normalized\")\n</code></pre>"},{"location":"api/core/#pipeline-execution-flow","title":"Pipeline Execution Flow","text":"<ol> <li>The <code>PipelineExecutor</code> parses the configuration file using <code>ConfigParser</code></li> <li>Global settings are stored in the <code>DataContext</code></li> <li>Each module is executed in sequence:</li> <li>The module class is dynamically imported based on the module type</li> <li>An instance of the module is created with the specified name and parameters</li> <li>Required inputs are validated</li> <li>The module's <code>run</code> method is called with the data context</li> <li>Outputs are validated</li> <li>If checkpointing is enabled, a checkpoint is created</li> <li>If any module fails, execution stops and returns <code>False</code></li> <li>If all modules complete successfully, execution returns <code>True</code></li> </ol>"},{"location":"api/core/#checkpointing-and-error-recovery","title":"Checkpointing and Error Recovery","text":"<p>The pipeline supports checkpointing to save and restore state between runs. This allows for recovery from failures without having to restart the entire pipeline.</p> <p>To enable checkpointing, configure the checkpointing section in your configuration:</p> <pre><code>pipeline:\n  # ... other settings ...\n  checkpointing:\n    enabled: true\n    modules_to_checkpoint: all  # or a list of module names\n    max_checkpoints: 5 # leaving this at 1 will always save only the last successfully run module's checkpoint\n</code></pre> <p>To resume from a checkpoint, use the <code>start_from</code> parameter when running the pipeline:</p> <pre><code>executor = PipelineExecutor(\"path/to/config.yaml\")\nexecutor.run(start_from=\"after_module1\")\n</code></pre>"},{"location":"api/core/#module-development-guidelines","title":"Module Development Guidelines","text":"<p>When developing new modules for the pipeline, follow these guidelines:</p> <ol> <li>Inherit from <code>AnalysisModule</code></li> <li>Define <code>PARAMETER_SCHEMA</code> to specify parameters and their validation rules</li> <li>Define <code>required_inputs</code> and <code>outputs</code> in the <code>__init__</code> method</li> <li>Always define a logger instance: <code>self.logger = logging.getLogger(f\"Module.{name}\")</code></li> <li>Implement the <code>run(data_context)</code> method to perform the module's functionality</li> <li>Handle exceptions and log appropriate messages</li> <li>Return <code>True</code> if successful, <code>False</code> otherwise</li> </ol> <p>Example implementation:</p> <pre><code>import logging\nfrom sc_pipeline.core.module import AnalysisModule\n\nclass MyModule(AnalysisModule):\n    \"\"\"A custom module for the pipeline.\"\"\"\n\n    PARAMETER_SCHEMA = {\n        'param1': {\n            'type': str,\n            'required': True,\n            'description': 'Description of parameter 1'\n        },\n        'param2': {\n            'type': int,\n            'default': 10,\n            'description': 'Description of parameter 2'\n        }\n    }\n\n    def __init__(self, name, params):\n        super().__init__(name, params)\n        self.logger = logging.getLogger(f\"Module.{name}\")\n        self.required_inputs = [\"input1\"]\n        self.outputs = [\"output1\"]\n\n    def run(self, data_context):\n        try:\n            # Get inputs\n            input_data = data_context.get(\"input1\")\n\n            # Get parameters\n            param1 = self.params.get('param1')\n            param2 = self.params.get('param2', 10)\n\n            self.logger.info(f\"Processing data with parameters: {param1}, {param2}\")\n\n            # Process data\n            output_data = self._process_data(input_data, param1, param2)\n\n            # Store outputs\n            data_context.set(\"output1\", output_data)\n\n            self.logger.info(\"Processing completed successfully\")\n            return True\n\n        except Exception as e:\n            self.logger.error(f\"Error in processing: {e}\", exc_info=True)\n            return False\n\n    def _process_data(self, data, param1, param2):\n        # Implementation of data processing\n        pass\n</code></pre>"},{"location":"api/core/#troubleshooting-and-debugging","title":"Troubleshooting and Debugging","text":""},{"location":"api/core/#logging","title":"Logging","text":"<p>The pipeline uses Python's logging module to provide detailed logs for troubleshooting. Adjust the log level to see more or less detail:</p> <pre><code>import logging\n\n# Set log level for detailed output\nlogging.basicConfig(level=logging.DEBUG)\n\n# For less detailed output\nlogging.basicConfig(level=logging.INFO)\n</code></pre> <p>Logger hierarchy: - <code>ConfigParser</code>: Messages related to configuration parsing - <code>DataContext</code>: Messages related to data management and checkpointing - <code>PipelineExecutor</code>: Messages related to pipeline execution - <code>Module.&lt;name&gt;</code>: Messages from specific modules (e.g., <code>Module.data_loading</code>)</p>"},{"location":"api/core/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"api/core/#configuration-file-structure","title":"Configuration File Structure","text":"<p>The configuration file controls the entire pipeline. Here's a more detailed explanation of its structure:</p> <pre><code>pipeline:\n  name: my_pipeline                # Name of the pipeline\n  output_dir: ./output            # Directory for output files\n  r_memory_limit_gb: 8            # Memory limit for R scripts\n  figure_defaults:                # Default figure settings\n    width: 8\n    height: 6\n  checkpointing:                  # Checkpointing configuration\n    enabled: true\n    modules_to_checkpoint: all    # or a list of module names\n    max_checkpoints: 5\n\nmodules:                          # List of modules to execute\n  - name: module1                 # Name of this module instance\n    type: ModuleType              # Type of module (class name)\n    params:                       # Module-specific parameters\n      param1: value1\n      param2: value2\n\n  - name: module2\n    type: AnotherModuleType\n    params:\n      param1: value1\n</code></pre>"},{"location":"api/core/#dynamic-module-loading","title":"Dynamic Module Loading","text":"<p>The pipeline automatically tries to import module classes based on their type. For example, if the configuration specifies <code>type: DataLoading</code>, the executor will try to import:</p> <p><pre><code>from sc_pipeline.modules.dataloading import DataLoading\n</code></pre> Currently, the pipeline only supports loading modules that exist within the modules directory. The PipelineExecutor class supports a <code>register_module_type</code> method that can be passed a <code>module_type</code> parameter used as a key to return the <code>module_class</code> parameter. This would allow registering modules outside of the module folder, but it is as of yet unimplemented. For now, custom modules are best placed in the module directory.</p>"},{"location":"api/core/#report-generation","title":"Report Generation","text":"<p>The pipeline includes a <code>ReportGenerator</code> module that can generate Markdown and HTML reports from the results of other modules. To add content to the report, modules can add figures using the <code>data_context.add_figure()</code> method:</p> <p><pre><code>data_context.add_figure(\n    module_name=self.name,\n    title=\"Figure Title\",\n    description=\"Description of the figure\",\n    image_path=img_path,\n    caption=\"Figure caption\"\n)\n</code></pre> See the DataContext section above for details</p>"},{"location":"api/core/#best-practices","title":"Best Practices","text":"<ol> <li>Immutability: Modules should avoid modifying input data directly, instead creating modified copies or using layers</li> <li>Error Handling: Always catch exceptions and log appropriate messages</li> <li>Validation: Validate inputs and parameters before processing</li> <li>Documentation: Provide detailed docstrings and parameter descriptions</li> <li>Logging: Use the logger to provide informative messages at appropriate levels</li> </ol>"},{"location":"api/core/#conclusion","title":"Conclusion","text":"<p>The Anatalyst core provides a flexible and extensible framework for building analysis pipelines that leverage both R and Python libraries. By understanding these core components, you can effectively develop new modules and customize the pipeline for specific research needs.</p> <p>For more information on specific modules, see the Module Documentation.</p>"},{"location":"api/utils/","title":"TBD","text":""},{"location":"configuration/","title":"Configuration Overview","text":"<p>Anatalyst uses YAML configuration files to define the pipeline structure and module parameters. This flexible approach allows you to customize the analysis workflow without changing code.</p>"},{"location":"configuration/#configuration-file-structure","title":"Configuration File Structure","text":"<p>A typical configuration file has two main sections:</p> <ol> <li>Pipeline Settings: Global settings for the entire pipeline</li> <li>Modules: List of modules to execute, with their parameters</li> </ol> <pre><code>pipeline:\n  name: my_analysis_pipeline\n  output_dir: ./output\n  r_memory_limit_gb: 8\n  figure_defaults:\n    width: 8\n    height: 6\n  checkpointing:\n    enabled: true\n    modules_to_checkpoint: all\n    max_checkpoints: 5\n\nmodules:\n  - name: module1\n    type: ModuleType1\n    params:\n      param1: value1\n      param2: value2\n\n  - name: module2\n    type: ModuleType2\n    params:\n      param1: value1\n      param2: value2\n</code></pre>"},{"location":"configuration/#pipeline-settings","title":"Pipeline Settings","text":"<p>The <code>pipeline</code> section contains global settings that apply to the entire pipeline:</p> Setting Type Description <code>name</code> string Name of the pipeline (used in reports) <code>output_dir</code> string Directory where results will be saved <code>r_memory_limit_gb</code> number Memory limit for R processes (in GB) <code>figure_defaults</code> object Default settings for generated figures <code>checkpointing</code> object Configuration for checkpoint system"},{"location":"configuration/#figure-defaults","title":"Figure Defaults","text":"<p>The <code>figure_defaults</code> section controls the default size and appearance of generated figures:</p> <pre><code>figure_defaults:\n  width: 8    # Width in inches\n  height: 6   # Height in inches\n</code></pre>"},{"location":"configuration/#checkpointing","title":"Checkpointing","text":"<p>The <code>checkpointing</code> section configures the checkpoint system, which allows resuming a pipeline from intermediate points:</p> <pre><code>checkpointing:\n  enabled: true                  # Whether checkpointing is enabled\n  modules_to_checkpoint: all     # Which modules to create checkpoints for\n  max_checkpoints: 5             # Maximum number of checkpoints to keep\n</code></pre> <p>For <code>modules_to_checkpoint</code>, you can specify: - <code>all</code>: Checkpoint after every module (default) - A list of module names: <code>['data_loading', 'filtering']</code></p>"},{"location":"configuration/#module-configuration","title":"Module Configuration","text":"<p>Each entry in the <code>modules</code> list defines a module to execute in the pipeline:</p> <pre><code>- name: module_name              # A unique name for this module instance\n  type: ModuleType               # The type of module to use (class name)\n  params:                        # Module-specific parameters\n    param1: value1\n    param2: value2\n</code></pre>"},{"location":"configuration/#module-execution-order","title":"Module Execution Order","text":"<p>Modules are executed in the order they appear in the configuration file. Each module:</p> <ol> <li>Receives the data context from previous modules</li> <li>Applies its analysis steps</li> <li>Updates the data context for subsequent modules</li> </ol>"},{"location":"configuration/#module-parameters","title":"Module Parameters","text":"<p>Each module has its own set of parameters, defined in the <code>params</code> section. Refer to the individual module documentation for details on available parameters.</p>"},{"location":"configuration/#validation","title":"Validation","text":"<p>The pipeline configuration is validated when loaded:</p> <ol> <li>Required sections (<code>pipeline</code> and <code>modules</code>) must be present</li> <li>The <code>pipeline</code> section must have a <code>name</code> and <code>output_dir</code></li> <li>Each module must have a <code>name</code> and <code>type</code></li> <li>Module parameters are validated against each module's parameter schema</li> </ol>"},{"location":"configuration/#example-configuration","title":"Example Configuration","text":"<p>Here's a minimal example configuration:</p> <pre><code>pipeline:\n  name: minimal_pipeline\n  output_dir: ./output\n\nmodules:\n  - name: data_loading\n    type: DataLoading\n    params:\n      file_path: /path/to/data.h5\n\n  - name: qc_metrics\n    type: QCMetrics\n    params:\n      mito_pattern: \"^MT-\"\n      ribo_pattern: \"^RP[SL]\"\n\n  - name: report_generator\n    type: ReportGenerator\n</code></pre> <p>For more complex examples, see the Examples page.</p>"},{"location":"configuration/#command-line-options","title":"Command Line Options","text":"<p>When running the pipeline, you can specify additional options:</p> <pre><code>python -m /workspace/scripts/run_pipeline.py --config my_config.yaml --checkpoint module_name --log-file pipeline.log\n</code></pre> <p>Options: - <code>--config</code>: Path to the configuration file (required) - <code>--checkpoint</code>: Resume from a specific checkpoint - <code>--log-file</code>: Path to save log output - <code>--log-level</code>: Logging level (DEBUG, INFO, WARNING, ERROR)</p>"},{"location":"configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Browse example configurations to see complete pipeline setups</li> <li>Check the module documentation for details on available modules and their parameters</li> <li>Learn how to create custom modules to extend the pipeline</li> </ul>"},{"location":"configuration/examples/","title":"TBD","text":""},{"location":"customization/","title":"TBD","text":""},{"location":"customization/new-modules/","title":"Creating Custom Modules for the Single-Cell Pipeline","text":""},{"location":"customization/new-modules/#overview","title":"Overview","text":"<p>Anatalyst is designed to be extensible, allowing you to create custom modules that integrate seamlessly with the existing workflow or create an entirely new workflow. This guide will walk you through the process of creating a new analysis module.</p>"},{"location":"customization/new-modules/#module-structure","title":"Module Structure","text":"<p>Each module is a Python class that inherits from <code>AnalysisModule</code>. The basic structure includes:</p> <ul> <li>Initialization method</li> <li>Parameter schema</li> <li><code>run</code> method</li> <li>Optional helper methods</li> </ul>"},{"location":"customization/new-modules/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"customization/new-modules/#1-create-the-module-file","title":"1. Create the Module File","text":"<p>Create a new file in <code>sc_pipeline/modules/</code> with a descriptive name, e.g., <code>myanalysis.py</code>.</p>"},{"location":"customization/new-modules/#2-import-required-modules","title":"2. Import Required Modules","text":"<pre><code># At a minimum\nimport logging\nfrom sc_pipeline.core.module import AnalysisModule\n</code></pre>"},{"location":"customization/new-modules/#3-define-the-module-class","title":"3. Define the Module Class","text":"<pre><code>class MyAnalysis(AnalysisModule):\n    \"\"\"\n    Description of your custom module's purpose.\n    \"\"\"\n\n    # Define the parameter schema\n    PARAMETER_SCHEMA = {\n        'param1': {\n            'type': str,  # Parameter type\n            'required': True,  # Whether the parameter is mandatory\n            'description': 'Description of the parameter'\n        },\n        'optional_param': {\n            'type': int,\n            'default': 10,  # Default value if not provided\n            'description': 'An optional parameter'\n        }\n    }\n\n    def __init__(self, name, params):\n        # Call the parent class constructor\n        super().__init__(name, params)\n\n        # Set up logging\n        self.logger = logging.getLogger(f\"Module.{name}\")\n\n        # Define required inputs and outputs\n        self.required_inputs = [\"data\"]  # Inputs this module needs\n        self.outputs = [\"data\"]  # Outputs this module will produce\n</code></pre>"},{"location":"customization/new-modules/#4-implement-the-run-method","title":"4. Implement the <code>run</code> Method","text":"<pre><code>def run(self, data_context):\n    \"\"\"\n    Main method to execute the module's analysis.\n\n    Args:\n        data_context: Shared data context containing pipeline data\n\n    Returns:\n        bool: True if successful, False otherwise\n    \"\"\"\n    try:\n        # Retrieve the AnnData object\n        adata = data_context.get(\"data\")\n\n        # Access module parameters\n        param1 = self.params.get('param1')\n        optional_param = self.params.get('optional_param', 10)\n\n        # Perform your analysis\n        # Example: Do something with the data\n\n        # Optional: Create visualization\n        if self.params.get('create_plots', True):\n            self._create_plots(adata, data_context)\n\n        # Update the data context\n        data_context.set(\"data\", adata)\n\n        return True\n\n    except Exception as e:\n        self.logger.error(f\"Error in analysis: {e}\", exc_info=True)\n        return False\n</code></pre>"},{"location":"customization/new-modules/#5-add-optional-visualization-method","title":"5. Add Optional Visualization Method","text":"<pre><code>def _create_plots(self, adata, data_context):\n    \"\"\"\n    Create and save visualization figures.\n\n    Args:\n        adata: AnnData object\n        data_context: Shared data context\n    \"\"\"\n    try:\n        # Create a matplotlib figure\n        fig, ax = plt.subplots(figsize=(8, 6))\n\n        # Create your plot\n        # sc.pl.something(adata, ax=ax)\n\n        # Save the figure using the parent class method\n        img_path = self.save_figure(data_context, self.name, fig)\n\n        # Add figure to the report\n        data_context.add_figure(\n            module_name=self.name,\n            title=\"My Analysis Plot\",\n            description=\"Description of the plot\",\n            image_path=img_path\n        )\n\n    except Exception as e:\n        self.logger.warning(f\"Error creating plots: {e}\")\n</code></pre>"},{"location":"customization/new-modules/#module-best-practices","title":"Module Best Practices","text":"<ul> <li>Use logging for tracking module progress and errors</li> <li>Handle exceptions gracefully</li> <li>Provide clear documentation and parameter descriptions</li> <li>Create visualizations when possible</li> <li>Minimize data transformation, preferring to add information to the AnnData object</li> </ul>"},{"location":"customization/new-modules/#using-your-custom-module","title":"Using Your Custom Module","text":"<p>To use your new module in a pipeline configuration:</p> <pre><code>modules:\n  - name: my_custom_analysis\n    type: MyAnalysis\n    params:\n      param1: \"example_value\"\n      optional_param: 20\n</code></pre>"},{"location":"customization/new-modules/#notes","title":"Notes","text":"<ul> <li>Modules should be idempotent (can be run multiple times without side effects)</li> <li>Prefer adding information to AnnData's <code>.obs</code>, <code>.var</code>, <code>.uns</code>, or as layers</li> <li>Keep modules focused on a single type of analysis</li> </ul>"},{"location":"examples/","title":"TBD","text":""},{"location":"examples/pbmc/","title":"TBD","text":""},{"location":"modules/","title":"Overview","text":""},{"location":"modules/#module-dependencies","title":"Module Dependencies","text":"<p>Some modules depend on the output of other modules. View the details of each module's indivual page to make sure they are implemented in an order that makes sense!</p>"},{"location":"modules/#module-interface","title":"Module Interface","text":"<p>All modules inherit from the <code>AnalysisModule</code> base class and implement the following key methods:</p> <ul> <li><code>__init__(name, params)</code>: Initializes the module with a name and parameters</li> <li><code>run(data_context)</code>: Executes the module's functionality on the data context</li> <li><code>validate_inputs(data_context)</code>: Ensures all required inputs are available</li> <li><code>validate_outputs(data_context)</code>: Ensures all expected outputs were created</li> </ul>"},{"location":"modules/#creating-custom-modules","title":"Creating Custom Modules","text":"<p>You can create custom modules by subclassing <code>AnalysisModule</code>. See the Creating New Modules guide for detailed instructions.</p>"},{"location":"modules/#configuring-modules","title":"Configuring Modules","text":"<p>Modules are configured through the pipeline configuration file using YAML syntax. All modules have a name and type. The name is customizable and designed to make logs and checkpoints clear to the user. The type is case-insensitive name of the module file; for example, <code>DataLoading</code> maps to <code>modules/dataloading.py</code>. Additionally, Each module has its own set of parameters, detailed in the individual module documentation.</p> <p>Example configuration for a module:</p> <pre><code>- name: filtering # Name that appears in logs and checkpoints\n  type: Filtering # maps to modules/filtering.py\n  params: # Parameters accepted by filtering module\n    filters:\n      n_genes_by_counts:\n        type: numeric\n        min: 200\n        max: 6000\n      pct_counts_mt:\n        type: numeric\n        max: 20\n    create_plots: true\n</code></pre> <p>Visit the individual module pages for detailed documentation on each module's parameters and functionality.</p>"},{"location":"modules/ambientrnaremoval/","title":"TBD","text":""},{"location":"modules/dataloading/","title":"DataLoading Module","text":"<p>The DataLoading module is the entry point for most single-cell RNA-seq analysis pipelines, responsible for loading aligned data from 10X Genomics Cell Ranger output files into the pipeline's data context. The module as it stands has very little flexibility for file types and formats, so feel free to augment this as needed!</p>"},{"location":"modules/dataloading/#overview","title":"Overview","text":"<p>This module uses Scanpy's <code>read_10x_h5</code> function to load data from 10X HDF5 files and prepares it for downstream analysis. It handles the initial data setup, including making variable names unique and creating the first data layer for tracking data transformations throughout the pipeline.</p> <p>Multimodal Data</p> <p>Right now, this only supports single modal data. Any other layers (i.e. Antibody Capture) are discarded Future implementations will shift from <code>AnnData</code> to using <code>muon</code> and <code>MuData</code> to resolve this</p>"},{"location":"modules/dataloading/#parameters","title":"Parameters","text":"Parameter Type Required Default Description <code>file_path</code> string Yes - Path to the 10X h5 file (typically <code>sample_filtered_feature_bc_matrix.h5</code>)"},{"location":"modules/dataloading/#example-configuration","title":"Example Configuration","text":"<pre><code>- name: data_loading\n  type: DataLoading\n  params:\n    file_path: /path/to/filtered_feature_bc_matrix.h5\n</code></pre>"},{"location":"modules/dataloading/#inputoutput","title":"Input/Output","text":""},{"location":"modules/dataloading/#inputs","title":"Inputs","text":"<ul> <li>None (this is typically the first module in a pipeline)</li> </ul>"},{"location":"modules/dataloading/#outputs","title":"Outputs","text":"<ul> <li><code>data</code>: An AnnData object containing the loaded single-cell data with:</li> <li><code>.X</code>: The main expression matrix</li> <li><code>.var_names</code>: Gene identifiers (made unique)</li> <li><code>.obs_names</code>: Cell barcodes</li> <li><code>.layers['loaded_counts']</code>: A copy of the original counts data</li> </ul>"},{"location":"modules/dataloading/#functionality","title":"Functionality","text":"<p>The module performs the following operations:</p> <ol> <li>File Validation: Checks that the specified file path exists and is accessible</li> <li>Data Loading: Uses Scanpy's <code>read_10x_h5</code> function to load the HDF5 file</li> <li>Variable Name Processing: Ensures all gene names are unique using Scanpy's <code>var_names_make_unique()</code> function</li> <li>Layer Creation: Creates a <code>loaded_counts</code> layer to preserve the original data before any transformations</li> <li>Data Context Storage: Stores the loaded AnnData object in the pipeline's data context for use by subsequent modules</li> </ol>"},{"location":"modules/dataloading/#supported-file-formats","title":"Supported File Formats","text":"<p>Currently, the DataLoading module supports:</p> <ul> <li>10X HDF5 files (<code>.h5</code>): The standard output format from Cell Ranger, typically named <code>sample_filtered_feature_bc_matrix.h5</code></li> </ul> <p>The module expects the standard 10X HDF5 structure containing: - Expression matrix (genes \u00d7 cells) - Gene information (features) - Cell barcode information</p>"},{"location":"modules/dataloading/#usage-notes","title":"Usage Notes","text":""},{"location":"modules/dataloading/#file-path-requirements","title":"File Path Requirements","text":"<ul> <li>The <code>file_path</code> parameter must point to a valid 10X HDF5 file</li> <li>Relative paths are supported and resolved relative to the working directory</li> <li>The file must be readable by the user running the pipeline (which should not be an issue when working inside the container)</li> </ul>"},{"location":"modules/dataloading/#data-structure","title":"Data Structure","text":"<p>After loading, the AnnData object contains: - Observations (<code>.obs</code>): Cell-level metadata (initially just cell barcodes) - Variables (<code>.var</code>): Gene-level metadata (gene IDs, symbols, etc.) - Expression Matrix (<code>.X</code>): Raw count data (typically sparse matrix format) - Layers: The <code>loaded_counts</code> layer preserves the original data</p>"},{"location":"modules/dataloading/#memory-considerations","title":"Memory Considerations","text":"<p>Large datasets may require significant memory. Consider the following: - 10X filtered matrices are typically manageable for most systems - Raw matrices (containing all droplets) may require more memory - This initially do not require large amounts of memory; however, the pipeline creates copies of data in layers, which increases memory usage, especially if some layers are not sparse</p>"},{"location":"modules/dataloading/#error-handling","title":"Error Handling","text":"<p>The module has minimal error handling, but will log a clear report for the source of the issue.</p>"},{"location":"modules/dataloading/#integration-with-other-modules","title":"Integration with Other Modules","text":"<p>The DataLoading module is designed to work seamlessly with other pipeline modules, providing the data for most downstream operations:</p> <ul> <li>Quality Control: The loaded data is immediately ready for QC metric calculation</li> <li>Ambient RNA Removal: Can work with both filtered and raw count matrices</li> <li>Preprocessing: All subsequent modules expect the AnnData structure created by this module</li> </ul>"},{"location":"modules/dataloading/#example-usage","title":"Example Usage","text":""},{"location":"modules/dataloading/#basic-usage","title":"Basic Usage","text":"<pre><code>modules:\n  - name: load_data\n    type: DataLoading\n    params:\n      file_path: ./data/filtered_feature_bc_matrix.h5\n</code></pre>"},{"location":"modules/dataloading/#with-absolute-path","title":"With Absolute Path","text":"<pre><code>modules:\n  - name: load_data\n    type: DataLoading\n    params:\n      file_path: /home/user/scrnaseq_data/sample1/outs/filtered_feature_bc_matrix.h5\n</code></pre>"},{"location":"modules/dataloading/#in-a-complete-pipeline","title":"In a Complete Pipeline","text":"<pre><code>pipeline:\n  name: full_analysis\n  output_dir: ./output\n\nmodules:\n  - name: data_loading\n    type: DataLoading\n    params:\n      file_path: ./data/filtered_feature_bc_matrix.h5\n\n  - name: qc_metrics\n    type: QCMetrics\n    params:\n      mito_pattern: \"^MT-\"\n\n  # Additional modules...\n</code></pre>"},{"location":"modules/dataloading/#implementation-details","title":"Implementation Details","text":"<p>The module leverages several key components:</p> <ul> <li>Scanpy Integration: Uses <code>sc.read_10x_h5()</code> for reliable 10X file parsing</li> <li>Layer Management: Utilizes the pipeline's <code>save_layer()</code> utility function to track data versions</li> <li>Error Logging: Comprehensive logging of all operations and errors</li> <li>Data Validation: Ensures loaded data meets pipeline requirements</li> </ul>"},{"location":"modules/dataloading/#output-verification","title":"Output Verification","text":"<p>After successful execution, you can verify the data was loaded correctly:</p> <pre><code># The loaded AnnData object should have:\nprint(f\"Loaded {adata.n_obs} cells and {adata.n_vars} genes\")\nprint(f\"Layers available: {list(adata.layers.keys())}\")\nprint(f\"Matrix type: {type(adata.X)}\")\n</code></pre> <p>Expected output: <pre><code>Loaded 8381 cells and 36601 genes\nLayers available: ['loaded_counts']\nMatrix type: &lt;class 'scipy.sparse._matrix.csr_matrix'&gt;\n</code></pre></p>"},{"location":"modules/dataloading/#troubleshooting","title":"Troubleshooting","text":""},{"location":"modules/dataloading/#common-issues","title":"Common Issues","text":"<p>File Not Found Error <pre><code>Error loading data: [Errno 2] No such file or directory: 'path/to/file.h5'\n</code></pre> - Verify the file path is correct - Check file permissions - Ensure the file exists and isn't corrupted</p> <p>Memory Errors <pre><code>MemoryError: Unable to allocate array\n</code></pre> - Monitor system memory usage - Consider using a machine with more RAM for large datasets, especially if using more memory-hungry modules after this one</p> <p>Invalid HDF5 Format <pre><code>OSError: Unable to open file (file signature not found)\n</code></pre> - Verify the file is a valid HDF5 file - Check if the file download completed successfully - Re-download the file if necessary</p>"},{"location":"modules/dataloading/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Loading Time: Varies depending on the file size, but generally a few seconds at most</li> <li>Storage: Creates additional layers that increase memory footprint</li> </ul>"},{"location":"modules/dataloading/#see-also","title":"See Also","text":"<ul> <li>QCMetrics Module: For calculating quality control metrics on loaded data</li> <li>AmbientRNARemoval Module: For removing background contamination (requires both raw and filtered files)</li> <li>Scanpy Documentation: For more details on the underlying data loading function</li> </ul>"},{"location":"modules/dimensionalityreduction/","title":"TBD","text":""},{"location":"modules/doubletdetection/","title":"TBD","text":""},{"location":"modules/filtering/","title":"TBD","text":""},{"location":"modules/pearsonnormalization/","title":"TBD","text":""},{"location":"modules/qcmetrics/","title":"QCMetrics Module","text":"<p>The QCMetrics module calculates quality control metrics for single-cell RNA-seq data, including gene and UMI counts per cell, as well as the percentage of reads mapping to mitochondrial and ribosomal genes.</p>"},{"location":"modules/qcmetrics/#overview","title":"Overview","text":"<p>Quality control (QC) is a critical step in single-cell RNA-seq analysis. This module uses Scanpy's <code>pp.calculate_qc_metrics</code> function to compute standard QC metrics and, optionally, generates visualizations of the distributions.</p>"},{"location":"modules/qcmetrics/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>mito_pattern</code> string <code>^MT-</code> Regex pattern to identify mitochondrial genes <code>ribo_pattern</code> string <code>^RP[SL]</code> Regex pattern to identify ribosomal genes <code>create_plots</code> boolean <code>True</code> Whether to generate plots of QC metrics <code>plot_title</code> string None Title for generated plots <code>layer_key</code> string None If provided, calculate metrics on this layer instead of <code>.X</code>"},{"location":"modules/qcmetrics/#example-configuration","title":"Example Configuration","text":"<pre><code>- name: pre_qc\n  type: QCMetrics\n  params:\n    mito_pattern: \"^MT-\"\n    ribo_pattern: \"^RP[SL]\"\n    create_plots: true\n    plot_title: \"Pre-filtering QC Metrics\"\n</code></pre>"},{"location":"modules/qcmetrics/#inputoutput","title":"Input/Output","text":""},{"location":"modules/qcmetrics/#inputs","title":"Inputs","text":"<ul> <li><code>data</code>: An AnnData object containing single-cell data</li> </ul>"},{"location":"modules/qcmetrics/#outputs","title":"Outputs","text":"<ul> <li>Updates the <code>data</code> object with:</li> <li><code>.obs['n_genes_by_counts']</code>: Number of genes with positive counts in each cell</li> <li><code>.obs['total_counts']</code>: Total counts per cell</li> <li><code>.obs['pct_counts_mt']</code>: Percentage of counts in mitochondrial genes</li> <li><code>.obs['pct_counts_ribo']</code>: Percentage of counts in ribosomal genes</li> <li>Visualization of these metrics in the report (if <code>create_plots</code> is <code>True</code>)</li> </ul>"},{"location":"modules/qcmetrics/#functionality","title":"Functionality","text":"<p>The module performs the following operations:</p> <ol> <li>Calculates basic QC metrics (genes and UMIs per cell)</li> <li>Identifies mitochondrial genes based on the provided pattern</li> <li>Calculates the percentage of reads mapping to mitochondrial genes per cell</li> <li>Identifies ribosomal genes based on the provided pattern</li> <li>Calculates the percentage of reads mapping to ribosomal genes per cell</li> <li>Optionally generates violin plots showing the distribution of these metrics</li> </ol>"},{"location":"modules/qcmetrics/#visualization","title":"Visualization","text":"<p>When <code>create_plots</code> is set to <code>True</code>, the module generates violin plots for:</p> <ul> <li>Number of genes detected per cell</li> <li>Total UMI counts per cell</li> <li>Percentage of counts from mitochondrial genes</li> <li>Percentage of counts from ribosomal genes</li> </ul> <p>These plots are saved as images and included in the final report.</p> <p></p>"},{"location":"modules/qcmetrics/#usage-notes","title":"Usage Notes","text":"<ul> <li>This module may be run twice in a pipeline: once before filtering to identify outliers and again after filtering to confirm the effectiveness of filtering steps.</li> <li>Different organisms may require adjusting the <code>mito_pattern</code> and <code>ribo_pattern</code>. For example:</li> <li>Human: <code>^MT-</code> (mitochondrial), <code>^RP[SL]</code> (ribosomal)</li> <li>Mouse: <code>^mt-</code> (mitochondrial), <code>^Rp[sl]</code> (ribosomal)</li> <li>High mitochondrial percentage often indicates cell stress or apoptosis</li> <li>High ribosomal percentage may indicate cell stress or high protein synthesis activity</li> </ul>"},{"location":"modules/qcmetrics/#implementation-details","title":"Implementation Details","text":"<p>The module uses Scanpy's <code>pp.calculate_qc_metrics</code> function, which computes:</p> <ul> <li>Number of genes detected per cell</li> <li>Total counts per cell</li> <li>Percent of counts in feature subsets (e.g., mitochondrial genes)</li> </ul> <p>The module's visualization uses Scanpy's <code>pl.violin</code> functionality for creating the multi-panel violin plots.</p>"},{"location":"modules/qcmetrics/#see-also","title":"See Also","text":"<ul> <li>Filtering Module: For filtering cells based on QC metrics</li> <li>Scanpy QC Documentation: For more details on the underlying implementation</li> </ul>"},{"location":"modules/reportgenerator/","title":"TBD","text":""}]}