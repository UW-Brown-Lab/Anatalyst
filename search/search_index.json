{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Anatalyst: The Analysis Catalyst - A Modular Pipeline built for Single-cell RNA-seq Analysis \u00b6 Anatalyst is a flexible, modular Python pipeline designed to facilitate boilerplate analytical workflows that leverage existing Python and R programs. The current scope of module support is focused on Single Cell RNA sequencing, built on top of Scanpy , providing a customizable workflow for common single-cell analysis tasks. Key Features \u00b6 Modular Architecture : Each analysis step is encapsulated in a module that can be included or excluded as needed Configurable Pipeline : Simple YAML configuration to customize pipeline parameters Checkpoint System : Save and resume pipeline execution from checkpoints R Integration : Seamless integration of R tools (like SoupX) for specialized analyses Reproducible Analysis : Detailed reports with visualizations and parameter settings Framework Flexibility : New modules can be custom-built and inserted into the pipeline to allow for any type of sequential analysis Workflow Overview \u00b6 Anatalyst provides a comprehensive workflow for single-cell analysis: Data Loading : Import aligned 10X single-cell data using an .h5 file Quality Control : Calculate QC metrics and visualize distributions Ambient RNA Removal : Remove background RNA contamination using SoupX Doublet Detection : Identify and flag potential cell doublets Cell Filtering : Filter out low-quality cells and outliers Pearson Normalization : Normalize data using Pearson residuals Dimensionality Reduction : PCA, UMAP, and t-SNE for visualization and analysis Report Generation : Create comprehensive HTML reports with key figures Quick Start \u00b6 TBD - expecting pull Docker container with pipeline already installed Mount local directory for input/output and extra module insertion? # Install the container docker pull something-or-other # Run a pipeline with a configuration file python -m sc_pipeline.scripts.run_pipeline --config my_config.yaml # This command will likely change and may just be the way the container is launched? # Perhaps these args just get passed to docker compose via ENV variables and we make a new entrypoint.sh file? Check out the Getting Started guide for more detailed instructions. Pipeline Diagram \u00b6 insert diagram here","title":"Home"},{"location":"#anatalyst-the-analysis-catalyst-a-modular-pipeline-built-for-single-cell-rna-seq-analysis","text":"Anatalyst is a flexible, modular Python pipeline designed to facilitate boilerplate analytical workflows that leverage existing Python and R programs. The current scope of module support is focused on Single Cell RNA sequencing, built on top of Scanpy , providing a customizable workflow for common single-cell analysis tasks.","title":"Anatalyst: The Analysis Catalyst - A Modular Pipeline built for Single-cell RNA-seq Analysis"},{"location":"#key-features","text":"Modular Architecture : Each analysis step is encapsulated in a module that can be included or excluded as needed Configurable Pipeline : Simple YAML configuration to customize pipeline parameters Checkpoint System : Save and resume pipeline execution from checkpoints R Integration : Seamless integration of R tools (like SoupX) for specialized analyses Reproducible Analysis : Detailed reports with visualizations and parameter settings Framework Flexibility : New modules can be custom-built and inserted into the pipeline to allow for any type of sequential analysis","title":"Key Features"},{"location":"#workflow-overview","text":"Anatalyst provides a comprehensive workflow for single-cell analysis: Data Loading : Import aligned 10X single-cell data using an .h5 file Quality Control : Calculate QC metrics and visualize distributions Ambient RNA Removal : Remove background RNA contamination using SoupX Doublet Detection : Identify and flag potential cell doublets Cell Filtering : Filter out low-quality cells and outliers Pearson Normalization : Normalize data using Pearson residuals Dimensionality Reduction : PCA, UMAP, and t-SNE for visualization and analysis Report Generation : Create comprehensive HTML reports with key figures","title":"Workflow Overview"},{"location":"#quick-start","text":"TBD - expecting pull Docker container with pipeline already installed Mount local directory for input/output and extra module insertion? # Install the container docker pull something-or-other # Run a pipeline with a configuration file python -m sc_pipeline.scripts.run_pipeline --config my_config.yaml # This command will likely change and may just be the way the container is launched? # Perhaps these args just get passed to docker compose via ENV variables and we make a new entrypoint.sh file? Check out the Getting Started guide for more detailed instructions.","title":"Quick Start"},{"location":"#pipeline-diagram","text":"insert diagram here","title":"Pipeline Diagram"},{"location":"getting-started/","text":"Getting Started \u00b6 This guide will walk you through running your first analysis with Anatalyst. Basic Usage \u00b6 Anatalyst is designed to be run from a configuration file that defines the modules to use and their parameters. The basic workflow is: Create a configuration file Run the pipeline Review the results Creating a Configuration File \u00b6 Create a YAML file that defines your pipeline. Here's a minimal example: pipeline : name : my_first_pipeline output_dir : ./output r_memory_limit_gb : 8 figure_defaults : width : 8 height : 6 modules : - name : data_loading type : DataLoading params : file_path : /path/to/your/filtered_feature_bc_matrix.h5 - name : qc_metrics type : QCMetrics params : mito_pattern : \"^MT-\" ribo_pattern : \"^RP[SL]\" - name : report_generator type : ReportGenerator Save this as minimal_config.yaml . Running the Pipeline \u00b6 Execute the pipeline using the run_pipeline.py script: python -m /workspace/scripts/run_pipeline.py --config minimal_config.yaml This will: Load your data Calculate QC metrics Generate a report in the output directory Example with a Complete Analysis \u00b6 Here's a more comprehensive example configuration for a full analysis workflow: pipeline : name : complete_analysis output_dir : ./output r_memory_limit_gb : 8 figure_defaults : width : 8 height : 6 checkpointing : enabled : true modules_to_checkpoint : all max_checkpoints : 5 modules : - name : data_loading type : DataLoading params : file_path : /path/to/filtered_feature_bc_matrix.h5 - name : pre_qc type : QCMetrics params : mito_pattern : \"^MT-\" ribo_pattern : \"^RP[SL]\" create_plots : true - name : ambient_removal type : AmbientRNARemoval params : raw_counts_path : /path/to/raw_feature_bc_matrix.h5 filtered_counts_path : /path/to/filtered_feature_bc_matrix.h5 ndims : 30 resolution : 0.8 - name : doublet_detection type : DoubletDetection params : expected_doublet_rate : 0.05 - name : filtering type : Filtering params : filters : n_genes_by_counts : type : numeric pct_counts_mt : type : numeric predicted_doublet : type : boolean - name : post_qc type : QCMetrics params : mito_pattern : \"^MT-\" ribo_pattern : \"^RP[SL]\" - name : normalization type : PearsonNormalization params : n_top_genes : 2000 - name : dim_reduction type : DimensionalityReduction params : n_pcs : 50 compute_umap : true compute_tsne : true - name : report_generator type : ReportGenerator params : generate_html : true Save this as complete_config.yaml and run it as before: python -m /workspace/scripts/run_pipeline.py --config complete_config.yaml Resuming from a Checkpoint \u00b6 If your pipeline fails or stops for any reason, you can resume from the last checkpoint: python -m /workspace/scripts/run_pipeline.py --config complete_config.yaml --checkpoint doublet_detection This will resume the pipeline from after the \"doublet_detection\" module. Viewing the Results \u00b6 After running the pipeline, check the output directory: output/ \u251c\u2500\u2500 checkpoints/ # Pipeline checkpoints \u251c\u2500\u2500 images/ # Generated figures \u251c\u2500\u2500 analysis_report.md # Markdown report \u2514\u2500\u2500 analysis_report.html # HTML report Open the HTML report in a web browser to view the analysis results, including visualizations and parameter settings. Next Steps \u00b6 Explore the Configuration documentation to learn about all available options Browse the Modules section to understand each analysis step in detail Check out the Examples for more use cases and sample analyses","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"This guide will walk you through running your first analysis with Anatalyst.","title":"Getting Started"},{"location":"getting-started/#basic-usage","text":"Anatalyst is designed to be run from a configuration file that defines the modules to use and their parameters. The basic workflow is: Create a configuration file Run the pipeline Review the results","title":"Basic Usage"},{"location":"getting-started/#creating-a-configuration-file","text":"Create a YAML file that defines your pipeline. Here's a minimal example: pipeline : name : my_first_pipeline output_dir : ./output r_memory_limit_gb : 8 figure_defaults : width : 8 height : 6 modules : - name : data_loading type : DataLoading params : file_path : /path/to/your/filtered_feature_bc_matrix.h5 - name : qc_metrics type : QCMetrics params : mito_pattern : \"^MT-\" ribo_pattern : \"^RP[SL]\" - name : report_generator type : ReportGenerator Save this as minimal_config.yaml .","title":"Creating a Configuration File"},{"location":"getting-started/#running-the-pipeline","text":"Execute the pipeline using the run_pipeline.py script: python -m /workspace/scripts/run_pipeline.py --config minimal_config.yaml This will: Load your data Calculate QC metrics Generate a report in the output directory","title":"Running the Pipeline"},{"location":"getting-started/#example-with-a-complete-analysis","text":"Here's a more comprehensive example configuration for a full analysis workflow: pipeline : name : complete_analysis output_dir : ./output r_memory_limit_gb : 8 figure_defaults : width : 8 height : 6 checkpointing : enabled : true modules_to_checkpoint : all max_checkpoints : 5 modules : - name : data_loading type : DataLoading params : file_path : /path/to/filtered_feature_bc_matrix.h5 - name : pre_qc type : QCMetrics params : mito_pattern : \"^MT-\" ribo_pattern : \"^RP[SL]\" create_plots : true - name : ambient_removal type : AmbientRNARemoval params : raw_counts_path : /path/to/raw_feature_bc_matrix.h5 filtered_counts_path : /path/to/filtered_feature_bc_matrix.h5 ndims : 30 resolution : 0.8 - name : doublet_detection type : DoubletDetection params : expected_doublet_rate : 0.05 - name : filtering type : Filtering params : filters : n_genes_by_counts : type : numeric pct_counts_mt : type : numeric predicted_doublet : type : boolean - name : post_qc type : QCMetrics params : mito_pattern : \"^MT-\" ribo_pattern : \"^RP[SL]\" - name : normalization type : PearsonNormalization params : n_top_genes : 2000 - name : dim_reduction type : DimensionalityReduction params : n_pcs : 50 compute_umap : true compute_tsne : true - name : report_generator type : ReportGenerator params : generate_html : true Save this as complete_config.yaml and run it as before: python -m /workspace/scripts/run_pipeline.py --config complete_config.yaml","title":"Example with a Complete Analysis"},{"location":"getting-started/#resuming-from-a-checkpoint","text":"If your pipeline fails or stops for any reason, you can resume from the last checkpoint: python -m /workspace/scripts/run_pipeline.py --config complete_config.yaml --checkpoint doublet_detection This will resume the pipeline from after the \"doublet_detection\" module.","title":"Resuming from a Checkpoint"},{"location":"getting-started/#viewing-the-results","text":"After running the pipeline, check the output directory: output/ \u251c\u2500\u2500 checkpoints/ # Pipeline checkpoints \u251c\u2500\u2500 images/ # Generated figures \u251c\u2500\u2500 analysis_report.md # Markdown report \u2514\u2500\u2500 analysis_report.html # HTML report Open the HTML report in a web browser to view the analysis results, including visualizations and parameter settings.","title":"Viewing the Results"},{"location":"getting-started/#next-steps","text":"Explore the Configuration documentation to learn about all available options Browse the Modules section to understand each analysis step in detail Check out the Examples for more use cases and sample analyses","title":"Next Steps"},{"location":"installation/","text":"Installation \u00b6 This guide walks you through the installation process for Anatalyst. ALL OF THIS IS WIP AND WILL NEED TO BE UPDATED Installation Methods \u00b6 Method 1: Using docker (Recommended) \u00b6 # Pull the Docker image docker pull ghcr.io/yourusername/downstream-scrnaseq:latest Using devcontainer (For Development) \u00b6 If you're using Visual Studio Code, a devcontainer configuration is provided in the .devcontainer folder. This allows you to develop within a container that has all dependencies pre-installed. Install the Remote Development extension pack in VS Code Open the repository folder in VS Code When prompted, click \"Reopen in Container\" VS Code will build the container and provide you with a fully configured development environment Troubleshooting \u00b6 Common Issues \u00b6 R Bridge Connection Issues \u00b6 If the R bridge fails to connect: Error in R bridge: /bin/sh: 1: Rscript: not found Make sure R is installed and the Rscript executable is in your PATH. Memory Errors with Large Datasets \u00b6 If you encounter memory errors when processing large datasets: MemoryError : Unable to allocate array with shape ... Try increasing the memory limit in your configuration file: pipeline : r_memory_limit_gb : 16 # Increase this value For additional issues, please check the GitHub Issues (dead link for now) page or submit a new issue.","title":"Installation"},{"location":"installation/#installation","text":"This guide walks you through the installation process for Anatalyst. ALL OF THIS IS WIP AND WILL NEED TO BE UPDATED","title":"Installation"},{"location":"installation/#installation-methods","text":"","title":"Installation Methods"},{"location":"installation/#method-1-using-docker-recommended","text":"# Pull the Docker image docker pull ghcr.io/yourusername/downstream-scrnaseq:latest","title":"Method 1: Using docker (Recommended)"},{"location":"installation/#using-devcontainer-for-development","text":"If you're using Visual Studio Code, a devcontainer configuration is provided in the .devcontainer folder. This allows you to develop within a container that has all dependencies pre-installed. Install the Remote Development extension pack in VS Code Open the repository folder in VS Code When prompted, click \"Reopen in Container\" VS Code will build the container and provide you with a fully configured development environment","title":"Using devcontainer (For Development)"},{"location":"installation/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"installation/#common-issues","text":"","title":"Common Issues"},{"location":"installation/#r-bridge-connection-issues","text":"If the R bridge fails to connect: Error in R bridge: /bin/sh: 1: Rscript: not found Make sure R is installed and the Rscript executable is in your PATH.","title":"R Bridge Connection Issues"},{"location":"installation/#memory-errors-with-large-datasets","text":"If you encounter memory errors when processing large datasets: MemoryError : Unable to allocate array with shape ... Try increasing the memory limit in your configuration file: pipeline : r_memory_limit_gb : 16 # Increase this value For additional issues, please check the GitHub Issues (dead link for now) page or submit a new issue.","title":"Memory Errors with Large Datasets"},{"location":"about/","text":"TBD \u00b6","title":"About"},{"location":"about/#tbd","text":"","title":"TBD"},{"location":"api/","text":"TBD \u00b6","title":"Overview"},{"location":"api/#tbd","text":"","title":"TBD"},{"location":"api/core/","text":"Anatalyst Pipeline Core API \u00b6 This document provides detailed documentation for the core components of the Anatalyst framework. The core modules form the foundation of the pipeline system, handling tasks such as configuration parsing, data management, module execution, and pipeline orchestration. Overview \u00b6 Analyst is designed around a modular architecture that allows for flexible composition of analysis workflows. Each analysis is broken down into a series of modules that each perform a specific task with the data. The core components facilitate this by: Configuration Management : Reading and validating YAML configuration files that define pipeline steps Data Context : Managing shared data between modules and providing checkpointing capabilities Module System : Defining the base interface for all analysis modules Pipeline Execution : Orchestrating the execution of modules in sequence Core Components \u00b6 ConfigParser \u00b6 The ConfigParser class handles the reading and validation of YAML configuration files that define pipeline structure and parameters. from sc_pipeline.core.config import ConfigParser parser = ConfigParser () config = parser . parse ( \"path/to/config.yaml\" ) Key Methods \u00b6 parse(config_file) : Parses a YAML configuration file and returns the validated configuration dictionary. _validate_config(config) : Internal method that validates the structure of the configuration. _set_defaults(config) : Internal method that sets default values for optional configuration parameters. Configuration Structure \u00b6 A basic configuration file should have the following structure: pipeline : name : my_pipeline output_dir : ./output r_memory_limit_gb : 8 # Optional figure_defaults : # Optional width : 8 height : 6 checkpointing : # Optional enabled : true modules_to_checkpoint : all # or a list of module names max_checkpoints : 5 modules : - name : module1 type : ModuleType params : param1 : value1 param2 : value2 - name : module2 type : AnotherModuleType params : param1 : value1 Scope \u00b6 In order to provide module level access to the pipeline's configuration, the executor adds the entire parsed configuration to the DataContext shortly after its instantiation before any modules are run. This can be accessed from inside any module using data_context.get('CONFIG') DataContext \u00b6 The DataContext class provides a shared storage space for data passed between modules in the pipeline. It also offers checkpointing capabilities to save and restore pipeline state. Anything handled inside of a module that is not explicitly written to a file space or to the DataContext will be unavailable from sc_pipeline.core.data_context import DataContext # Create a data context with checkpointing enabled context = DataContext ( checkpoint_dir = \"./checkpoints\" , max_checkpoints = 3 ) # Store and retrieve data context . set ( \"key\" , value ) value = context . get ( \"key\" ) # Save and load checkpoints context . save_checkpoint ( \"after_module1\" ) context . load_checkpoint ( \"after_module1\" ) Key Methods \u00b6 set(key, value) : Store data by key get(key, default=None) : Retrieve data by key, with an optional default value __contains__(key) : Check if a key exists ( key in context ) keys() : Get all available data keys save_checkpoint(checkpoint_name) : Save the current state to a checkpoint file load_checkpoint(checkpoint_name) : Load data from a checkpoint file add_figure(module_name, title=None, description=None, image_path=None, caption=None) : Add a figure to be included in reports AnalysisModule \u00b6 The AnalysisModule class is the base class for all analysis modules in the pipeline. It defines the interface that modules must implement and provides common functionality. from sc_pipeline.core.module import AnalysisModule class MyModule ( AnalysisModule ): \"\"\"Custom module implementation.\"\"\" PARAMETER_SCHEMA = { 'param1' : { 'type' : str , 'required' : True , 'description' : 'Description of parameter 1' }, 'param2' : { 'type' : int , 'default' : 10 , 'description' : 'Description of parameter 2' } } def __init__ ( self , name , params ): super () . __init__ ( name , params ) self . required_inputs = [ \"input1\" , \"input2\" ] self . outputs = [ \"output1\" ] def run ( self , data_context ): # Implementation of module functionality # Access parameters with self.params.get('param_name', default_value) # Access inputs with data_context.get('input_name') # Store outputs with data_context.set('output_name', value) return True # Return True if successful, False otherwise Key Methods \u00b6 __init__(name, params) : Initialize the module with a name and parameters run(data_context) : Execute the module's analysis (must be implemented by subclasses) validate_inputs(data_context) : Check if all required inputs are available validate_outputs(data_context) : Check if all expected outputs were created _validate_params(provided_params) : Validate parameters against schema and apply defaults get_metadata() : Return metadata about the module save_figure(data_context, module_name, fig, figsize=None, name=None, output_dir=None, dpi=300) : Save a matplotlib figure Parameter Schema Definition \u00b6 Module parameters can be defined using the PARAMETER_SCHEMA class variable. This schema is used to validate parameters and apply defaults. Each parameter entry should include: type : The expected Python type (e.g., str , int , float , bool , list , dict ) required : Whether the parameter is required (default: False ) default : Default value if not provided (optional) description : Human-readable description of the parameter (optional) For list parameters, you can specify the expected element type using element_type . PipelineExecutor \u00b6 The PipelineExecutor class handles the orchestration of pipeline execution, running modules in sequence according to the configuration. from sc_pipeline.core.executor import PipelineExecutor # Create and run a pipeline from a configuration file executor = PipelineExecutor ( \"path/to/config.yaml\" ) success = executor . run () # Optionally, start from a checkpoint success = executor . run ( start_from = \"after_module1\" ) Key Methods \u00b6 __init__(config_file) : Initialize the executor with a configuration file register_module_type(module_type, module_class) : Register a module type with its implementing class run(start_from=None) : Execute the pipeline, optionally starting from a checkpoint _get_module_class(module_type) : Internal method to get the class for a module type _set_global_settings() : Internal method to store global pipeline settings Utility Modules \u00b6 RBridge \u00b6 The RBridge class provides a bridge for calling R functions from Python, using a temporary workspace directory for file exchange. from sc_pipeline.utils.r_bridge import RBridge # Create an R bridge with a specified memory limit r_bridge = RBridge ( r_script_dir = \"./r_scripts\" , memory_limit_gb = 16 ) # Run an R script with arguments success , stdout , stderr = r_bridge . run_r_script ( \"script.R\" , { \"arg1\" : \"value1\" , \"arg2\" : \"value2\" }) # Get the path to a file in the workspace file_path = r_bridge . get_workspace_path ( \"output.txt\" ) # Clean up when done r_bridge . cleanup_workspace () AnnData Utilities \u00b6 The adata_utils module provides utilities for working with AnnData objects, particularly focused on managing layers. from sc_pipeline.utils.adata_utils import save_layer , set_active_layer # Save data as a layer in AnnData adata = save_layer ( adata , name = \"raw_counts\" , data = None , make_active = False ) # Set a specific layer as the active layer (X matrix) adata = set_active_layer ( adata , layer_name = \"normalized\" ) Pipeline Execution Flow \u00b6 The PipelineExecutor parses the configuration file using ConfigParser Global settings are stored in the DataContext Each module is executed in sequence: The module class is dynamically imported based on the module type An instance of the module is created with the specified name and parameters Required inputs are validated The module's run method is called with the data context Outputs are validated If checkpointing is enabled, a checkpoint is created If any module fails, execution stops and returns False If all modules complete successfully, execution returns True Checkpointing and Error Recovery \u00b6 The pipeline supports checkpointing to save and restore state between runs. This allows for recovery from failures without having to restart the entire pipeline. To enable checkpointing, configure the checkpointing section in your configuration: pipeline : # ... other settings ... checkpointing : enabled : true modules_to_checkpoint : all # or a list of module names max_checkpoints : 5 To resume from a checkpoint, use the start_from parameter when running the pipeline: executor = PipelineExecutor ( \"path/to/config.yaml\" ) executor . run ( start_from = \"after_module1\" ) Module Development Guidelines \u00b6 When developing new modules for the pipeline, follow these guidelines: Inherit from AnalysisModule Define PARAMETER_SCHEMA to specify parameters and their validation rules Define required_inputs and outputs in the __init__ method Implement the run(data_context) method to perform the module's functionality Handle exceptions and log appropriate messages Return True if successful, False otherwise Example implementation: import logging from sc_pipeline.core.module import AnalysisModule class MyModule ( AnalysisModule ): \"\"\"A custom module for the pipeline.\"\"\" PARAMETER_SCHEMA = { 'param1' : { 'type' : str , 'required' : True , 'description' : 'Description of parameter 1' }, 'param2' : { 'type' : int , 'default' : 10 , 'description' : 'Description of parameter 2' } } def __init__ ( self , name , params ): super () . __init__ ( name , params ) self . logger = logging . getLogger ( f \"Module. { name } \" ) self . required_inputs = [ \"input1\" ] self . outputs = [ \"output1\" ] def run ( self , data_context ): try : # Get inputs input_data = data_context . get ( \"input1\" ) # Get parameters param1 = self . params . get ( 'param1' ) param2 = self . params . get ( 'param2' , 10 ) self . logger . info ( f \"Processing data with parameters: { param1 } , { param2 } \" ) # Process data output_data = self . _process_data ( input_data , param1 , param2 ) # Store outputs data_context . set ( \"output1\" , output_data ) self . logger . info ( \"Processing completed successfully\" ) return True except Exception as e : self . logger . error ( f \"Error in processing: { e } \" , exc_info = True ) return False def _process_data ( self , data , param1 , param2 ): # Implementation of data processing pass Troubleshooting and Debugging \u00b6 Common Issues \u00b6 Module not found : Ensure the module class is in the correct path ( sc_pipeline.modules.moduletype ) or register it manually with register_module_type . Missing inputs : Check that previous modules correctly set required outputs and that output keys match the required input keys. Parameter validation failures : Check the parameter schema and ensure parameters match the expected types and constraints. Checkpoint loading failures : Ensure the checkpoint exists and is compatible with the current configuration. Logging \u00b6 The pipeline uses Python's logging module to provide detailed logs for troubleshooting. Adjust the log level to see more or less detail: import logging # Set log level for detailed output logging . basicConfig ( level = logging . DEBUG ) # For less detailed output logging . basicConfig ( level = logging . INFO ) Logger hierarchy: - ConfigParser : Messages related to configuration parsing - DataContext : Messages related to data management and checkpointing - PipelineExecutor : Messages related to pipeline execution - Module.<name> : Messages from specific modules (e.g., Module.data_loading ) Advanced Configuration \u00b6 Configuration File Structure \u00b6 The configuration file controls the entire pipeline. Here's a more detailed explanation of its structure: pipeline : name : my_pipeline # Name of the pipeline output_dir : ./output # Directory for output files r_memory_limit_gb : 8 # Memory limit for R scripts figure_defaults : # Default figure settings width : 8 height : 6 checkpointing : # Checkpointing configuration enabled : true modules_to_checkpoint : all # or a list of module names max_checkpoints : 5 modules : # List of modules to execute - name : module1 # Name of this module instance type : ModuleType # Type of module (class name) params : # Module-specific parameters param1 : value1 param2 : value2 - name : module2 type : AnotherModuleType params : param1 : value1 Dynamic Module Loading \u00b6 The pipeline automatically tries to import module classes based on their type. For example, if the configuration specifies type: DataLoading , the executor will try to import: from sc_pipeline.modules.dataloading import DataLoading If you have modules outside the standard location, you can register them manually: from custom_module import CustomModule executor = PipelineExecutor ( \"path/to/config.yaml\" ) executor . register_module_type ( \"CustomType\" , CustomModule ) executor . run () Report Generation \u00b6 The pipeline includes a ReportGenerator module that can generate Markdown and HTML reports from the results of other modules. To add content to the report, modules can add figures using the data_context.add_figure() method: data_context . add_figure ( module_name = self . name , title = \"Figure Title\" , description = \"Description of the figure\" , image_path = img_path , caption = \"Figure caption\" ) Best Practices \u00b6 Immutability : Modules should avoid modifying input data directly, instead creating modified copies or using layers Error Handling : Always catch exceptions and log appropriate messages Validation : Validate inputs and parameters before processing Documentation : Provide detailed docstrings and parameter descriptions Logging : Use the logger to provide informative messages at appropriate levels Conclusion \u00b6 The SC Pipeline core provides a flexible and extensible framework for building single-cell RNA-seq analysis pipelines. By understanding these core components, you can effectively develop new modules and customize the pipeline for specific research needs. For more information on specific modules, see the Module Documentation .","title":"Core Classes"},{"location":"api/core/#anatalyst-pipeline-core-api","text":"This document provides detailed documentation for the core components of the Anatalyst framework. The core modules form the foundation of the pipeline system, handling tasks such as configuration parsing, data management, module execution, and pipeline orchestration.","title":"Anatalyst Pipeline Core API"},{"location":"api/core/#overview","text":"Analyst is designed around a modular architecture that allows for flexible composition of analysis workflows. Each analysis is broken down into a series of modules that each perform a specific task with the data. The core components facilitate this by: Configuration Management : Reading and validating YAML configuration files that define pipeline steps Data Context : Managing shared data between modules and providing checkpointing capabilities Module System : Defining the base interface for all analysis modules Pipeline Execution : Orchestrating the execution of modules in sequence","title":"Overview"},{"location":"api/core/#core-components","text":"","title":"Core Components"},{"location":"api/core/#configparser","text":"The ConfigParser class handles the reading and validation of YAML configuration files that define pipeline structure and parameters. from sc_pipeline.core.config import ConfigParser parser = ConfigParser () config = parser . parse ( \"path/to/config.yaml\" )","title":"ConfigParser"},{"location":"api/core/#key-methods","text":"parse(config_file) : Parses a YAML configuration file and returns the validated configuration dictionary. _validate_config(config) : Internal method that validates the structure of the configuration. _set_defaults(config) : Internal method that sets default values for optional configuration parameters.","title":"Key Methods"},{"location":"api/core/#configuration-structure","text":"A basic configuration file should have the following structure: pipeline : name : my_pipeline output_dir : ./output r_memory_limit_gb : 8 # Optional figure_defaults : # Optional width : 8 height : 6 checkpointing : # Optional enabled : true modules_to_checkpoint : all # or a list of module names max_checkpoints : 5 modules : - name : module1 type : ModuleType params : param1 : value1 param2 : value2 - name : module2 type : AnotherModuleType params : param1 : value1","title":"Configuration Structure"},{"location":"api/core/#scope","text":"In order to provide module level access to the pipeline's configuration, the executor adds the entire parsed configuration to the DataContext shortly after its instantiation before any modules are run. This can be accessed from inside any module using data_context.get('CONFIG')","title":"Scope"},{"location":"api/core/#datacontext","text":"The DataContext class provides a shared storage space for data passed between modules in the pipeline. It also offers checkpointing capabilities to save and restore pipeline state. Anything handled inside of a module that is not explicitly written to a file space or to the DataContext will be unavailable from sc_pipeline.core.data_context import DataContext # Create a data context with checkpointing enabled context = DataContext ( checkpoint_dir = \"./checkpoints\" , max_checkpoints = 3 ) # Store and retrieve data context . set ( \"key\" , value ) value = context . get ( \"key\" ) # Save and load checkpoints context . save_checkpoint ( \"after_module1\" ) context . load_checkpoint ( \"after_module1\" )","title":"DataContext"},{"location":"api/core/#key-methods_1","text":"set(key, value) : Store data by key get(key, default=None) : Retrieve data by key, with an optional default value __contains__(key) : Check if a key exists ( key in context ) keys() : Get all available data keys save_checkpoint(checkpoint_name) : Save the current state to a checkpoint file load_checkpoint(checkpoint_name) : Load data from a checkpoint file add_figure(module_name, title=None, description=None, image_path=None, caption=None) : Add a figure to be included in reports","title":"Key Methods"},{"location":"api/core/#analysismodule","text":"The AnalysisModule class is the base class for all analysis modules in the pipeline. It defines the interface that modules must implement and provides common functionality. from sc_pipeline.core.module import AnalysisModule class MyModule ( AnalysisModule ): \"\"\"Custom module implementation.\"\"\" PARAMETER_SCHEMA = { 'param1' : { 'type' : str , 'required' : True , 'description' : 'Description of parameter 1' }, 'param2' : { 'type' : int , 'default' : 10 , 'description' : 'Description of parameter 2' } } def __init__ ( self , name , params ): super () . __init__ ( name , params ) self . required_inputs = [ \"input1\" , \"input2\" ] self . outputs = [ \"output1\" ] def run ( self , data_context ): # Implementation of module functionality # Access parameters with self.params.get('param_name', default_value) # Access inputs with data_context.get('input_name') # Store outputs with data_context.set('output_name', value) return True # Return True if successful, False otherwise","title":"AnalysisModule"},{"location":"api/core/#key-methods_2","text":"__init__(name, params) : Initialize the module with a name and parameters run(data_context) : Execute the module's analysis (must be implemented by subclasses) validate_inputs(data_context) : Check if all required inputs are available validate_outputs(data_context) : Check if all expected outputs were created _validate_params(provided_params) : Validate parameters against schema and apply defaults get_metadata() : Return metadata about the module save_figure(data_context, module_name, fig, figsize=None, name=None, output_dir=None, dpi=300) : Save a matplotlib figure","title":"Key Methods"},{"location":"api/core/#parameter-schema-definition","text":"Module parameters can be defined using the PARAMETER_SCHEMA class variable. This schema is used to validate parameters and apply defaults. Each parameter entry should include: type : The expected Python type (e.g., str , int , float , bool , list , dict ) required : Whether the parameter is required (default: False ) default : Default value if not provided (optional) description : Human-readable description of the parameter (optional) For list parameters, you can specify the expected element type using element_type .","title":"Parameter Schema Definition"},{"location":"api/core/#pipelineexecutor","text":"The PipelineExecutor class handles the orchestration of pipeline execution, running modules in sequence according to the configuration. from sc_pipeline.core.executor import PipelineExecutor # Create and run a pipeline from a configuration file executor = PipelineExecutor ( \"path/to/config.yaml\" ) success = executor . run () # Optionally, start from a checkpoint success = executor . run ( start_from = \"after_module1\" )","title":"PipelineExecutor"},{"location":"api/core/#key-methods_3","text":"__init__(config_file) : Initialize the executor with a configuration file register_module_type(module_type, module_class) : Register a module type with its implementing class run(start_from=None) : Execute the pipeline, optionally starting from a checkpoint _get_module_class(module_type) : Internal method to get the class for a module type _set_global_settings() : Internal method to store global pipeline settings","title":"Key Methods"},{"location":"api/core/#utility-modules","text":"","title":"Utility Modules"},{"location":"api/core/#rbridge","text":"The RBridge class provides a bridge for calling R functions from Python, using a temporary workspace directory for file exchange. from sc_pipeline.utils.r_bridge import RBridge # Create an R bridge with a specified memory limit r_bridge = RBridge ( r_script_dir = \"./r_scripts\" , memory_limit_gb = 16 ) # Run an R script with arguments success , stdout , stderr = r_bridge . run_r_script ( \"script.R\" , { \"arg1\" : \"value1\" , \"arg2\" : \"value2\" }) # Get the path to a file in the workspace file_path = r_bridge . get_workspace_path ( \"output.txt\" ) # Clean up when done r_bridge . cleanup_workspace ()","title":"RBridge"},{"location":"api/core/#anndata-utilities","text":"The adata_utils module provides utilities for working with AnnData objects, particularly focused on managing layers. from sc_pipeline.utils.adata_utils import save_layer , set_active_layer # Save data as a layer in AnnData adata = save_layer ( adata , name = \"raw_counts\" , data = None , make_active = False ) # Set a specific layer as the active layer (X matrix) adata = set_active_layer ( adata , layer_name = \"normalized\" )","title":"AnnData Utilities"},{"location":"api/core/#pipeline-execution-flow","text":"The PipelineExecutor parses the configuration file using ConfigParser Global settings are stored in the DataContext Each module is executed in sequence: The module class is dynamically imported based on the module type An instance of the module is created with the specified name and parameters Required inputs are validated The module's run method is called with the data context Outputs are validated If checkpointing is enabled, a checkpoint is created If any module fails, execution stops and returns False If all modules complete successfully, execution returns True","title":"Pipeline Execution Flow"},{"location":"api/core/#checkpointing-and-error-recovery","text":"The pipeline supports checkpointing to save and restore state between runs. This allows for recovery from failures without having to restart the entire pipeline. To enable checkpointing, configure the checkpointing section in your configuration: pipeline : # ... other settings ... checkpointing : enabled : true modules_to_checkpoint : all # or a list of module names max_checkpoints : 5 To resume from a checkpoint, use the start_from parameter when running the pipeline: executor = PipelineExecutor ( \"path/to/config.yaml\" ) executor . run ( start_from = \"after_module1\" )","title":"Checkpointing and Error Recovery"},{"location":"api/core/#module-development-guidelines","text":"When developing new modules for the pipeline, follow these guidelines: Inherit from AnalysisModule Define PARAMETER_SCHEMA to specify parameters and their validation rules Define required_inputs and outputs in the __init__ method Implement the run(data_context) method to perform the module's functionality Handle exceptions and log appropriate messages Return True if successful, False otherwise Example implementation: import logging from sc_pipeline.core.module import AnalysisModule class MyModule ( AnalysisModule ): \"\"\"A custom module for the pipeline.\"\"\" PARAMETER_SCHEMA = { 'param1' : { 'type' : str , 'required' : True , 'description' : 'Description of parameter 1' }, 'param2' : { 'type' : int , 'default' : 10 , 'description' : 'Description of parameter 2' } } def __init__ ( self , name , params ): super () . __init__ ( name , params ) self . logger = logging . getLogger ( f \"Module. { name } \" ) self . required_inputs = [ \"input1\" ] self . outputs = [ \"output1\" ] def run ( self , data_context ): try : # Get inputs input_data = data_context . get ( \"input1\" ) # Get parameters param1 = self . params . get ( 'param1' ) param2 = self . params . get ( 'param2' , 10 ) self . logger . info ( f \"Processing data with parameters: { param1 } , { param2 } \" ) # Process data output_data = self . _process_data ( input_data , param1 , param2 ) # Store outputs data_context . set ( \"output1\" , output_data ) self . logger . info ( \"Processing completed successfully\" ) return True except Exception as e : self . logger . error ( f \"Error in processing: { e } \" , exc_info = True ) return False def _process_data ( self , data , param1 , param2 ): # Implementation of data processing pass","title":"Module Development Guidelines"},{"location":"api/core/#troubleshooting-and-debugging","text":"","title":"Troubleshooting and Debugging"},{"location":"api/core/#common-issues","text":"Module not found : Ensure the module class is in the correct path ( sc_pipeline.modules.moduletype ) or register it manually with register_module_type . Missing inputs : Check that previous modules correctly set required outputs and that output keys match the required input keys. Parameter validation failures : Check the parameter schema and ensure parameters match the expected types and constraints. Checkpoint loading failures : Ensure the checkpoint exists and is compatible with the current configuration.","title":"Common Issues"},{"location":"api/core/#logging","text":"The pipeline uses Python's logging module to provide detailed logs for troubleshooting. Adjust the log level to see more or less detail: import logging # Set log level for detailed output logging . basicConfig ( level = logging . DEBUG ) # For less detailed output logging . basicConfig ( level = logging . INFO ) Logger hierarchy: - ConfigParser : Messages related to configuration parsing - DataContext : Messages related to data management and checkpointing - PipelineExecutor : Messages related to pipeline execution - Module.<name> : Messages from specific modules (e.g., Module.data_loading )","title":"Logging"},{"location":"api/core/#advanced-configuration","text":"","title":"Advanced Configuration"},{"location":"api/core/#configuration-file-structure","text":"The configuration file controls the entire pipeline. Here's a more detailed explanation of its structure: pipeline : name : my_pipeline # Name of the pipeline output_dir : ./output # Directory for output files r_memory_limit_gb : 8 # Memory limit for R scripts figure_defaults : # Default figure settings width : 8 height : 6 checkpointing : # Checkpointing configuration enabled : true modules_to_checkpoint : all # or a list of module names max_checkpoints : 5 modules : # List of modules to execute - name : module1 # Name of this module instance type : ModuleType # Type of module (class name) params : # Module-specific parameters param1 : value1 param2 : value2 - name : module2 type : AnotherModuleType params : param1 : value1","title":"Configuration File Structure"},{"location":"api/core/#dynamic-module-loading","text":"The pipeline automatically tries to import module classes based on their type. For example, if the configuration specifies type: DataLoading , the executor will try to import: from sc_pipeline.modules.dataloading import DataLoading If you have modules outside the standard location, you can register them manually: from custom_module import CustomModule executor = PipelineExecutor ( \"path/to/config.yaml\" ) executor . register_module_type ( \"CustomType\" , CustomModule ) executor . run ()","title":"Dynamic Module Loading"},{"location":"api/core/#report-generation","text":"The pipeline includes a ReportGenerator module that can generate Markdown and HTML reports from the results of other modules. To add content to the report, modules can add figures using the data_context.add_figure() method: data_context . add_figure ( module_name = self . name , title = \"Figure Title\" , description = \"Description of the figure\" , image_path = img_path , caption = \"Figure caption\" )","title":"Report Generation"},{"location":"api/core/#best-practices","text":"Immutability : Modules should avoid modifying input data directly, instead creating modified copies or using layers Error Handling : Always catch exceptions and log appropriate messages Validation : Validate inputs and parameters before processing Documentation : Provide detailed docstrings and parameter descriptions Logging : Use the logger to provide informative messages at appropriate levels","title":"Best Practices"},{"location":"api/core/#conclusion","text":"The SC Pipeline core provides a flexible and extensible framework for building single-cell RNA-seq analysis pipelines. By understanding these core components, you can effectively develop new modules and customize the pipeline for specific research needs. For more information on specific modules, see the Module Documentation .","title":"Conclusion"},{"location":"api/utils/","text":"TBD \u00b6","title":"Utilities"},{"location":"api/utils/#tbd","text":"","title":"TBD"},{"location":"configuration/","text":"Configuration Overview \u00b6 Anatalyst uses YAML configuration files to define the pipeline structure and module parameters. This flexible approach allows you to customize the analysis workflow without changing code. Configuration File Structure \u00b6 A typical configuration file has two main sections: Pipeline Settings : Global settings for the entire pipeline Modules : List of modules to execute, with their parameters pipeline : name : my_analysis_pipeline output_dir : ./output r_memory_limit_gb : 8 figure_defaults : width : 8 height : 6 checkpointing : enabled : true modules_to_checkpoint : all max_checkpoints : 5 modules : - name : module1 type : ModuleType1 params : param1 : value1 param2 : value2 - name : module2 type : ModuleType2 params : param1 : value1 param2 : value2 Pipeline Settings \u00b6 The pipeline section contains global settings that apply to the entire pipeline: Setting Type Description name string Name of the pipeline (used in reports) output_dir string Directory where results will be saved r_memory_limit_gb number Memory limit for R processes (in GB) figure_defaults object Default settings for generated figures checkpointing object Configuration for checkpoint system Figure Defaults \u00b6 The figure_defaults section controls the default size and appearance of generated figures: figure_defaults : width : 8 # Width in inches height : 6 # Height in inches Checkpointing \u00b6 The checkpointing section configures the checkpoint system, which allows resuming a pipeline from intermediate points: checkpointing : enabled : true # Whether checkpointing is enabled modules_to_checkpoint : all # Which modules to create checkpoints for max_checkpoints : 5 # Maximum number of checkpoints to keep For modules_to_checkpoint , you can specify: - all : Checkpoint after every module (default) - A list of module names: ['data_loading', 'filtering'] Module Configuration \u00b6 Each entry in the modules list defines a module to execute in the pipeline: - name : module_name # A unique name for this module instance type : ModuleType # The type of module to use (class name) params : # Module-specific parameters param1 : value1 param2 : value2 Module Execution Order \u00b6 Modules are executed in the order they appear in the configuration file. Each module: Receives the data context from previous modules Applies its analysis steps Updates the data context for subsequent modules Module Parameters \u00b6 Each module has its own set of parameters, defined in the params section. Refer to the individual module documentation for details on available parameters. Validation \u00b6 The pipeline configuration is validated when loaded: Required sections ( pipeline and modules ) must be present The pipeline section must have a name and output_dir Each module must have a name and type Module parameters are validated against each module's parameter schema Example Configuration \u00b6 Here's a minimal example configuration: pipeline : name : minimal_pipeline output_dir : ./output modules : - name : data_loading type : DataLoading params : file_path : /path/to/data.h5 - name : qc_metrics type : QCMetrics params : mito_pattern : \"^MT-\" ribo_pattern : \"^RP[SL]\" - name : report_generator type : ReportGenerator For more complex examples, see the Examples page. Command Line Options \u00b6 When running the pipeline, you can specify additional options: python -m /workspace/scripts/run_pipeline.py --config my_config.yaml --checkpoint module_name --log-file pipeline.log Options: - --config : Path to the configuration file (required) - --checkpoint : Resume from a specific checkpoint - --log-file : Path to save log output - --log-level : Logging level (DEBUG, INFO, WARNING, ERROR) Next Steps \u00b6 Browse example configurations to see complete pipeline setups Check the module documentation for details on available modules and their parameters Learn how to create custom modules to extend the pipeline","title":"Overview"},{"location":"configuration/#configuration-overview","text":"Anatalyst uses YAML configuration files to define the pipeline structure and module parameters. This flexible approach allows you to customize the analysis workflow without changing code.","title":"Configuration Overview"},{"location":"configuration/#configuration-file-structure","text":"A typical configuration file has two main sections: Pipeline Settings : Global settings for the entire pipeline Modules : List of modules to execute, with their parameters pipeline : name : my_analysis_pipeline output_dir : ./output r_memory_limit_gb : 8 figure_defaults : width : 8 height : 6 checkpointing : enabled : true modules_to_checkpoint : all max_checkpoints : 5 modules : - name : module1 type : ModuleType1 params : param1 : value1 param2 : value2 - name : module2 type : ModuleType2 params : param1 : value1 param2 : value2","title":"Configuration File Structure"},{"location":"configuration/#pipeline-settings","text":"The pipeline section contains global settings that apply to the entire pipeline: Setting Type Description name string Name of the pipeline (used in reports) output_dir string Directory where results will be saved r_memory_limit_gb number Memory limit for R processes (in GB) figure_defaults object Default settings for generated figures checkpointing object Configuration for checkpoint system","title":"Pipeline Settings"},{"location":"configuration/#figure-defaults","text":"The figure_defaults section controls the default size and appearance of generated figures: figure_defaults : width : 8 # Width in inches height : 6 # Height in inches","title":"Figure Defaults"},{"location":"configuration/#checkpointing","text":"The checkpointing section configures the checkpoint system, which allows resuming a pipeline from intermediate points: checkpointing : enabled : true # Whether checkpointing is enabled modules_to_checkpoint : all # Which modules to create checkpoints for max_checkpoints : 5 # Maximum number of checkpoints to keep For modules_to_checkpoint , you can specify: - all : Checkpoint after every module (default) - A list of module names: ['data_loading', 'filtering']","title":"Checkpointing"},{"location":"configuration/#module-configuration","text":"Each entry in the modules list defines a module to execute in the pipeline: - name : module_name # A unique name for this module instance type : ModuleType # The type of module to use (class name) params : # Module-specific parameters param1 : value1 param2 : value2","title":"Module Configuration"},{"location":"configuration/#module-execution-order","text":"Modules are executed in the order they appear in the configuration file. Each module: Receives the data context from previous modules Applies its analysis steps Updates the data context for subsequent modules","title":"Module Execution Order"},{"location":"configuration/#module-parameters","text":"Each module has its own set of parameters, defined in the params section. Refer to the individual module documentation for details on available parameters.","title":"Module Parameters"},{"location":"configuration/#validation","text":"The pipeline configuration is validated when loaded: Required sections ( pipeline and modules ) must be present The pipeline section must have a name and output_dir Each module must have a name and type Module parameters are validated against each module's parameter schema","title":"Validation"},{"location":"configuration/#example-configuration","text":"Here's a minimal example configuration: pipeline : name : minimal_pipeline output_dir : ./output modules : - name : data_loading type : DataLoading params : file_path : /path/to/data.h5 - name : qc_metrics type : QCMetrics params : mito_pattern : \"^MT-\" ribo_pattern : \"^RP[SL]\" - name : report_generator type : ReportGenerator For more complex examples, see the Examples page.","title":"Example Configuration"},{"location":"configuration/#command-line-options","text":"When running the pipeline, you can specify additional options: python -m /workspace/scripts/run_pipeline.py --config my_config.yaml --checkpoint module_name --log-file pipeline.log Options: - --config : Path to the configuration file (required) - --checkpoint : Resume from a specific checkpoint - --log-file : Path to save log output - --log-level : Logging level (DEBUG, INFO, WARNING, ERROR)","title":"Command Line Options"},{"location":"configuration/#next-steps","text":"Browse example configurations to see complete pipeline setups Check the module documentation for details on available modules and their parameters Learn how to create custom modules to extend the pipeline","title":"Next Steps"},{"location":"configuration/examples/","text":"TBD \u00b6","title":"Examples"},{"location":"configuration/examples/#tbd","text":"","title":"TBD"},{"location":"customization/","text":"TBD \u00b6","title":"Overview"},{"location":"customization/#tbd","text":"","title":"TBD"},{"location":"customization/new-modules/","text":"Creating Custom Modules for the Single-Cell Pipeline \u00b6 Overview \u00b6 Anatalyst is designed to be extensible, allowing you to create custom modules that integrate seamlessly with the existing workflow or create an entirely new workflow. This guide will walk you through the process of creating a new analysis module. Module Structure \u00b6 Each module is a Python class that inherits from AnalysisModule . The basic structure includes: Initialization method Parameter schema run method Optional helper methods Step-by-Step Guide \u00b6 1. Create the Module File \u00b6 Create a new file in sc_pipeline/modules/ with a descriptive name, e.g., myanalysis.py . 2. Import Required Modules \u00b6 # At a minimum import logging from sc_pipeline.core.module import AnalysisModule 3. Define the Module Class \u00b6 class MyAnalysis ( AnalysisModule ): \"\"\" Description of your custom module's purpose. \"\"\" # Define the parameter schema PARAMETER_SCHEMA = { 'param1' : { 'type' : str , # Parameter type 'required' : True , # Whether the parameter is mandatory 'description' : 'Description of the parameter' }, 'optional_param' : { 'type' : int , 'default' : 10 , # Default value if not provided 'description' : 'An optional parameter' } } def __init__ ( self , name , params ): # Call the parent class constructor super () . __init__ ( name , params ) # Set up logging self . logger = logging . getLogger ( f \"Module. { name } \" ) # Define required inputs and outputs self . required_inputs = [ \"data\" ] # Inputs this module needs self . outputs = [ \"data\" ] # Outputs this module will produce 4. Implement the run Method \u00b6 def run ( self , data_context ): \"\"\" Main method to execute the module's analysis. Args: data_context: Shared data context containing pipeline data Returns: bool: True if successful, False otherwise \"\"\" try : # Retrieve the AnnData object adata = data_context . get ( \"data\" ) # Access module parameters param1 = self . params . get ( 'param1' ) optional_param = self . params . get ( 'optional_param' , 10 ) # Perform your analysis # Example: Do something with the data # Optional: Create visualization if self . params . get ( 'create_plots' , True ): self . _create_plots ( adata , data_context ) # Update the data context data_context . set ( \"data\" , adata ) return True except Exception as e : self . logger . error ( f \"Error in analysis: { e } \" , exc_info = True ) return False 5. Add Optional Visualization Method \u00b6 def _create_plots ( self , adata , data_context ): \"\"\" Create and save visualization figures. Args: adata: AnnData object data_context: Shared data context \"\"\" try : # Create a matplotlib figure fig , ax = plt . subplots ( figsize = ( 8 , 6 )) # Create your plot # sc.pl.something(adata, ax=ax) # Save the figure using the parent class method img_path = self . save_figure ( data_context , self . name , fig ) # Add figure to the report data_context . add_figure ( module_name = self . name , title = \"My Analysis Plot\" , description = \"Description of the plot\" , image_path = img_path ) except Exception as e : self . logger . warning ( f \"Error creating plots: { e } \" ) Module Best Practices \u00b6 Use logging for tracking module progress and errors Handle exceptions gracefully Provide clear documentation and parameter descriptions Create visualizations when possible Minimize data transformation, preferring to add information to the AnnData object Using Your Custom Module \u00b6 To use your new module in a pipeline configuration: modules : - name : my_custom_analysis type : MyAnalysis params : param1 : \"example_value\" optional_param : 20 Notes \u00b6 Modules should be idempotent (can be run multiple times without side effects) Prefer adding information to AnnData's .obs , .var , .uns , or as layers Keep modules focused on a single type of analysis","title":"Creating New Modules"},{"location":"customization/new-modules/#creating-custom-modules-for-the-single-cell-pipeline","text":"","title":"Creating Custom Modules for the Single-Cell Pipeline"},{"location":"customization/new-modules/#overview","text":"Anatalyst is designed to be extensible, allowing you to create custom modules that integrate seamlessly with the existing workflow or create an entirely new workflow. This guide will walk you through the process of creating a new analysis module.","title":"Overview"},{"location":"customization/new-modules/#module-structure","text":"Each module is a Python class that inherits from AnalysisModule . The basic structure includes: Initialization method Parameter schema run method Optional helper methods","title":"Module Structure"},{"location":"customization/new-modules/#step-by-step-guide","text":"","title":"Step-by-Step Guide"},{"location":"customization/new-modules/#1-create-the-module-file","text":"Create a new file in sc_pipeline/modules/ with a descriptive name, e.g., myanalysis.py .","title":"1. Create the Module File"},{"location":"customization/new-modules/#2-import-required-modules","text":"# At a minimum import logging from sc_pipeline.core.module import AnalysisModule","title":"2. Import Required Modules"},{"location":"customization/new-modules/#3-define-the-module-class","text":"class MyAnalysis ( AnalysisModule ): \"\"\" Description of your custom module's purpose. \"\"\" # Define the parameter schema PARAMETER_SCHEMA = { 'param1' : { 'type' : str , # Parameter type 'required' : True , # Whether the parameter is mandatory 'description' : 'Description of the parameter' }, 'optional_param' : { 'type' : int , 'default' : 10 , # Default value if not provided 'description' : 'An optional parameter' } } def __init__ ( self , name , params ): # Call the parent class constructor super () . __init__ ( name , params ) # Set up logging self . logger = logging . getLogger ( f \"Module. { name } \" ) # Define required inputs and outputs self . required_inputs = [ \"data\" ] # Inputs this module needs self . outputs = [ \"data\" ] # Outputs this module will produce","title":"3. Define the Module Class"},{"location":"customization/new-modules/#4-implement-the-run-method","text":"def run ( self , data_context ): \"\"\" Main method to execute the module's analysis. Args: data_context: Shared data context containing pipeline data Returns: bool: True if successful, False otherwise \"\"\" try : # Retrieve the AnnData object adata = data_context . get ( \"data\" ) # Access module parameters param1 = self . params . get ( 'param1' ) optional_param = self . params . get ( 'optional_param' , 10 ) # Perform your analysis # Example: Do something with the data # Optional: Create visualization if self . params . get ( 'create_plots' , True ): self . _create_plots ( adata , data_context ) # Update the data context data_context . set ( \"data\" , adata ) return True except Exception as e : self . logger . error ( f \"Error in analysis: { e } \" , exc_info = True ) return False","title":"4. Implement the run Method"},{"location":"customization/new-modules/#5-add-optional-visualization-method","text":"def _create_plots ( self , adata , data_context ): \"\"\" Create and save visualization figures. Args: adata: AnnData object data_context: Shared data context \"\"\" try : # Create a matplotlib figure fig , ax = plt . subplots ( figsize = ( 8 , 6 )) # Create your plot # sc.pl.something(adata, ax=ax) # Save the figure using the parent class method img_path = self . save_figure ( data_context , self . name , fig ) # Add figure to the report data_context . add_figure ( module_name = self . name , title = \"My Analysis Plot\" , description = \"Description of the plot\" , image_path = img_path ) except Exception as e : self . logger . warning ( f \"Error creating plots: { e } \" )","title":"5. Add Optional Visualization Method"},{"location":"customization/new-modules/#module-best-practices","text":"Use logging for tracking module progress and errors Handle exceptions gracefully Provide clear documentation and parameter descriptions Create visualizations when possible Minimize data transformation, preferring to add information to the AnnData object","title":"Module Best Practices"},{"location":"customization/new-modules/#using-your-custom-module","text":"To use your new module in a pipeline configuration: modules : - name : my_custom_analysis type : MyAnalysis params : param1 : \"example_value\" optional_param : 20","title":"Using Your Custom Module"},{"location":"customization/new-modules/#notes","text":"Modules should be idempotent (can be run multiple times without side effects) Prefer adding information to AnnData's .obs , .var , .uns , or as layers Keep modules focused on a single type of analysis","title":"Notes"},{"location":"examples/","text":"TBD \u00b6","title":"Overview"},{"location":"examples/#tbd","text":"","title":"TBD"},{"location":"examples/pbmc/","text":"TBD \u00b6","title":"PBMC Analysis"},{"location":"examples/pbmc/#tbd","text":"","title":"TBD"},{"location":"modules/","text":"Module Dependencies \u00b6 Some modules depend on the output of other modules. View the details of each module's indivual page to make sure they are implemented in an order that makes sense! Module Interface \u00b6 All modules inherit from the AnalysisModule base class and implement the following key methods: __init__(name, params) : Initializes the module with a name and parameters run(data_context) : Executes the module's functionality on the data context validate_inputs(data_context) : Ensures all required inputs are available validate_outputs(data_context) : Ensures all expected outputs were created Creating Custom Modules \u00b6 You can create custom modules by subclassing AnalysisModule . See the Creating New Modules guide for detailed instructions. Configuring Modules \u00b6 Modules are configured through the pipeline configuration file using YAML syntax. All modules have a name and type. The name is customizable and designed to make logs and checkpoints clear to the user. The type is case-insensitive name of the module file; for example, DataLoading maps to modules/dataloading.py . Additionally, Each module has its own set of parameters, detailed in the individual module documentation. Example configuration for a module: - name : filtering # Name that appears in logs and checkpoints type : Filtering # maps to modules/filtering.py params : # Parameters accepted by filtering module filters : n_genes_by_counts : type : numeric min : 200 max : 6000 pct_counts_mt : type : numeric max : 20 create_plots : true Visit the individual module pages for detailed documentation on each module's parameters and functionality.","title":"Overview"},{"location":"modules/#module-dependencies","text":"Some modules depend on the output of other modules. View the details of each module's indivual page to make sure they are implemented in an order that makes sense!","title":"Module Dependencies"},{"location":"modules/#module-interface","text":"All modules inherit from the AnalysisModule base class and implement the following key methods: __init__(name, params) : Initializes the module with a name and parameters run(data_context) : Executes the module's functionality on the data context validate_inputs(data_context) : Ensures all required inputs are available validate_outputs(data_context) : Ensures all expected outputs were created","title":"Module Interface"},{"location":"modules/#creating-custom-modules","text":"You can create custom modules by subclassing AnalysisModule . See the Creating New Modules guide for detailed instructions.","title":"Creating Custom Modules"},{"location":"modules/#configuring-modules","text":"Modules are configured through the pipeline configuration file using YAML syntax. All modules have a name and type. The name is customizable and designed to make logs and checkpoints clear to the user. The type is case-insensitive name of the module file; for example, DataLoading maps to modules/dataloading.py . Additionally, Each module has its own set of parameters, detailed in the individual module documentation. Example configuration for a module: - name : filtering # Name that appears in logs and checkpoints type : Filtering # maps to modules/filtering.py params : # Parameters accepted by filtering module filters : n_genes_by_counts : type : numeric min : 200 max : 6000 pct_counts_mt : type : numeric max : 20 create_plots : true Visit the individual module pages for detailed documentation on each module's parameters and functionality.","title":"Configuring Modules"},{"location":"modules/ambientrnaremoval/","text":"TBD \u00b6","title":"AmbientRNARemoval"},{"location":"modules/ambientrnaremoval/#tbd","text":"","title":"TBD"},{"location":"modules/dataloading/","text":"TBD \u00b6","title":"DataLoading"},{"location":"modules/dataloading/#tbd","text":"","title":"TBD"},{"location":"modules/dimensionalityreduction/","text":"TBD \u00b6","title":"DimensionalityReduction"},{"location":"modules/dimensionalityreduction/#tbd","text":"","title":"TBD"},{"location":"modules/doubletdetection/","text":"TBD \u00b6","title":"DoubletDetection"},{"location":"modules/doubletdetection/#tbd","text":"","title":"TBD"},{"location":"modules/filtering/","text":"TBD \u00b6","title":"Filtering"},{"location":"modules/filtering/#tbd","text":"","title":"TBD"},{"location":"modules/pearsonnormalization/","text":"TBD \u00b6","title":"PearsonNormalization"},{"location":"modules/pearsonnormalization/#tbd","text":"","title":"TBD"},{"location":"modules/qcmetrics/","text":"QCMetrics Module \u00b6 The QCMetrics module calculates quality control metrics for single-cell RNA-seq data, including gene and UMI counts per cell, as well as the percentage of reads mapping to mitochondrial and ribosomal genes. Overview \u00b6 Quality control (QC) is a critical step in single-cell RNA-seq analysis. This module uses Scanpy's pp.calculate_qc_metrics function to compute standard QC metrics and, optionally, generates visualizations of the distributions. Parameters \u00b6 Parameter Type Default Description mito_pattern string ^MT- Regex pattern to identify mitochondrial genes ribo_pattern string ^RP[SL] Regex pattern to identify ribosomal genes create_plots boolean True Whether to generate plots of QC metrics plot_title string None Title for generated plots layer_key string None If provided, calculate metrics on this layer instead of .X Example Configuration \u00b6 - name : pre_qc type : QCMetrics params : mito_pattern : \"^MT-\" ribo_pattern : \"^RP[SL]\" create_plots : true plot_title : \"Pre-filtering QC Metrics\" Input/Output \u00b6 Inputs \u00b6 data : An AnnData object containing single-cell data Outputs \u00b6 Updates the data object with: .obs['n_genes_by_counts'] : Number of genes with positive counts in each cell .obs['total_counts'] : Total counts per cell .obs['pct_counts_mt'] : Percentage of counts in mitochondrial genes .obs['pct_counts_ribo'] : Percentage of counts in ribosomal genes Visualization of these metrics in the report (if create_plots is True ) Functionality \u00b6 The module performs the following operations: Calculates basic QC metrics (genes and UMIs per cell) Identifies mitochondrial genes based on the provided pattern Calculates the percentage of reads mapping to mitochondrial genes per cell Identifies ribosomal genes based on the provided pattern Calculates the percentage of reads mapping to ribosomal genes per cell Optionally generates violin plots showing the distribution of these metrics Visualization \u00b6 When create_plots is set to True , the module generates violin plots for: Number of genes detected per cell Total UMI counts per cell Percentage of counts from mitochondrial genes Percentage of counts from ribosomal genes These plots are saved as images and included in the final report. Usage Notes \u00b6 This module may be run twice in a pipeline: once before filtering to identify outliers and again after filtering to confirm the effectiveness of filtering steps. Different organisms may require adjusting the mito_pattern and ribo_pattern . For example: Human: ^MT- (mitochondrial), ^RP[SL] (ribosomal) Mouse: ^mt- (mitochondrial), ^Rp[sl] (ribosomal) High mitochondrial percentage often indicates cell stress or apoptosis High ribosomal percentage may indicate cell stress or high protein synthesis activity Implementation Details \u00b6 The module uses Scanpy's pp.calculate_qc_metrics function, which computes: Number of genes detected per cell Total counts per cell Percent of counts in feature subsets (e.g., mitochondrial genes) The module's visualization uses Scanpy's pl.violin functionality for creating the multi-panel violin plots. See Also \u00b6 Filtering Module : For filtering cells based on QC metrics Scanpy QC Documentation : For more details on the underlying implementation API Reference \u00b6 ::: sc_pipeline.modules.qcmetrics.QCMetrics options: show_root_heading: true show_source: true","title":"QCMetrics"},{"location":"modules/qcmetrics/#qcmetrics-module","text":"The QCMetrics module calculates quality control metrics for single-cell RNA-seq data, including gene and UMI counts per cell, as well as the percentage of reads mapping to mitochondrial and ribosomal genes.","title":"QCMetrics Module"},{"location":"modules/qcmetrics/#overview","text":"Quality control (QC) is a critical step in single-cell RNA-seq analysis. This module uses Scanpy's pp.calculate_qc_metrics function to compute standard QC metrics and, optionally, generates visualizations of the distributions.","title":"Overview"},{"location":"modules/qcmetrics/#parameters","text":"Parameter Type Default Description mito_pattern string ^MT- Regex pattern to identify mitochondrial genes ribo_pattern string ^RP[SL] Regex pattern to identify ribosomal genes create_plots boolean True Whether to generate plots of QC metrics plot_title string None Title for generated plots layer_key string None If provided, calculate metrics on this layer instead of .X","title":"Parameters"},{"location":"modules/qcmetrics/#example-configuration","text":"- name : pre_qc type : QCMetrics params : mito_pattern : \"^MT-\" ribo_pattern : \"^RP[SL]\" create_plots : true plot_title : \"Pre-filtering QC Metrics\"","title":"Example Configuration"},{"location":"modules/qcmetrics/#inputoutput","text":"","title":"Input/Output"},{"location":"modules/qcmetrics/#inputs","text":"data : An AnnData object containing single-cell data","title":"Inputs"},{"location":"modules/qcmetrics/#outputs","text":"Updates the data object with: .obs['n_genes_by_counts'] : Number of genes with positive counts in each cell .obs['total_counts'] : Total counts per cell .obs['pct_counts_mt'] : Percentage of counts in mitochondrial genes .obs['pct_counts_ribo'] : Percentage of counts in ribosomal genes Visualization of these metrics in the report (if create_plots is True )","title":"Outputs"},{"location":"modules/qcmetrics/#functionality","text":"The module performs the following operations: Calculates basic QC metrics (genes and UMIs per cell) Identifies mitochondrial genes based on the provided pattern Calculates the percentage of reads mapping to mitochondrial genes per cell Identifies ribosomal genes based on the provided pattern Calculates the percentage of reads mapping to ribosomal genes per cell Optionally generates violin plots showing the distribution of these metrics","title":"Functionality"},{"location":"modules/qcmetrics/#visualization","text":"When create_plots is set to True , the module generates violin plots for: Number of genes detected per cell Total UMI counts per cell Percentage of counts from mitochondrial genes Percentage of counts from ribosomal genes These plots are saved as images and included in the final report.","title":"Visualization"},{"location":"modules/qcmetrics/#usage-notes","text":"This module may be run twice in a pipeline: once before filtering to identify outliers and again after filtering to confirm the effectiveness of filtering steps. Different organisms may require adjusting the mito_pattern and ribo_pattern . For example: Human: ^MT- (mitochondrial), ^RP[SL] (ribosomal) Mouse: ^mt- (mitochondrial), ^Rp[sl] (ribosomal) High mitochondrial percentage often indicates cell stress or apoptosis High ribosomal percentage may indicate cell stress or high protein synthesis activity","title":"Usage Notes"},{"location":"modules/qcmetrics/#implementation-details","text":"The module uses Scanpy's pp.calculate_qc_metrics function, which computes: Number of genes detected per cell Total counts per cell Percent of counts in feature subsets (e.g., mitochondrial genes) The module's visualization uses Scanpy's pl.violin functionality for creating the multi-panel violin plots.","title":"Implementation Details"},{"location":"modules/qcmetrics/#see-also","text":"Filtering Module : For filtering cells based on QC metrics Scanpy QC Documentation : For more details on the underlying implementation","title":"See Also"},{"location":"modules/qcmetrics/#api-reference","text":"::: sc_pipeline.modules.qcmetrics.QCMetrics options: show_root_heading: true show_source: true","title":"API Reference"},{"location":"modules/reportgenerator/","text":"TBD \u00b6","title":"ReportGenerator"},{"location":"modules/reportgenerator/#tbd","text":"","title":"TBD"}]}